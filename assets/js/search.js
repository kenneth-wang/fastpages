
var documents = [{
    "id": 0,
    "url": "https://kenneth-wang.github.io/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "https://kenneth-wang.github.io/about/",
    "title": "My title",
    "body": " AI Engineer (Apprentice) @ AI Singapore. Aspiring AI/ML Engineer Looking forward to empower others through my AI/ML learnings! "
    }, {
    "id": 2,
    "url": "https://kenneth-wang.github.io/categories/",
    "title": "Tags",
    "body": "Contents: {% if site. categories. size &gt; 0 %} {% for category in site. categories %} {% capture category_name %}{{ category | first }}{% endcapture %} {{ category_name }}{% endfor %}{% endif %} {% for category in site. categories %}  {% capture category_name %}{{ category | first }}{% endcapture %} &lt;h3 id = {{ category_name }} &gt;&lt;i class= fas fa-tags category-tags-icon &gt;&lt;/i&gt;&lt;/i&gt; {{ category_name }}&lt;/h3&gt;&lt;a name= {{ category_name | slugize }} &gt;&lt;/a&gt;{% for post in site. categories[category_name] %}{%- assign date_format = site. minima. date_format | default:  %b %-d, %Y  -%}&lt;article class= archive-item &gt; &lt;p class= post-meta post-meta-title &gt;&lt;a class= page-meta  href= {{ site. baseurl }}{{ post. url }} &gt;{{post. title}}&lt;/a&gt; • {{ post. date | date: date_format }}&lt;/p&gt;&lt;/article&gt;{% endfor %} {% endfor %}"
    }, {
    "id": 3,
    "url": "https://kenneth-wang.github.io/images/copied_from_nb/",
    "title": "",
    "body": "WarningDo not manually save images into this folder. This is used by GitHub Actions to automatically copy images.  Any images you save into this folder could be deleted at build time. "
    }, {
    "id": 4,
    "url": "https://kenneth-wang.github.io/2020/03/06/_03_01_Smartie_Slackbot.html",
    "title": "Smartie - Your personalized stock watchlist",
    "body": "2020/03/06 -           There is an abundance of websites/news app that are offering stock watchlists for users to get timely updates. However it may be difficult for us to find one that is customizable and able to offer information apart from price movements. By the end of this post, I hope you can create your own Slack bot that gives you personalized information based on your needs. We will be working through the steps together to create a bot that sends you a list of S&amp;P500 stocks and their Relative Strength Index (RSI) on a periodic basis. You can always amend the code if you are interested in other stock information. In terms of deployment, the app with all of its dependencies will be packaged into a docker image and be pushed to Docker Hub, a service provided by Docker to share docker images. With the docker image, we will build a docker container that includes everything we need to run the app on a server. For this example, I will be using Digital Ocean to host the app. Ok let's get started! Project directory structure The files that we will be referring to can be found here and in this section, I will illustrate how the different files will fit together. Before delving into the code and to avoid any confusion, let's look at how the files are arranged. The sp500_stocks. csv file contains the current constituents of the popular S&amp;P500 stock index. The classes/functions that we will be building can be found in stock. py and app. py.  The rest of the files will be used to build the docker image/container for us to deploy on the server.       smartie/_______ data/___________ sp500_stocks. csv_______ src/___________ __init__. py___________ stock. py_______ app. py_______ conda. yml_______ Dockerfile_______ credentials. py # To be created by user_______ env. list # To be created by user_______ run. sh    Generating the credentials we need Before we are able to send any stock related information to a Slack channel, we will need to retrieve some credentials/IDs from Alpha Vantage and Slack. We will store them in a credentials. py file to be used with this notebook. Click on the links below to setup the credentials. ALPHA_VANTAGE_KEY: We will be using Alpha Vantage to retrieve the stock prices. Ever since Yahoo decided to discontinue their finance API service,it has been difficult for many users to find a new platform to access pricing data for free. Alpha Vantage offers the data for free, however if you are looking to create a high-frequency trading bot, you will probably find yourself disappointed. There is a limit of 5 API-requests per minute and 500 API requests per day. However, you can always subscribe to their paid services if you are looking to remove these limits. API token: This is required to use Slack's API service for your selected Development Slack Workspace (channel). Follow the instructions in the link to obtain an access key. Channel ID: This is required to access the channel you have attached your app to. Slack Bot Token: Credentials required to use the bot associated with your channel. This can be accessed via the Slack API App webpage. Slack Signing Secret: A unique string that Slack shares with us. When sending a request, the signing secret is combined with the body using a HMAC-SHA256 keyed hash. The resulting signature is unique and is used by Slack to verify that the request is actually sent by you. This can be found under  Basic Information  on the side bar of the Slack API webpage.  Once you have the credentials, add it into your credentials. py file. Refer to below for an example.       SLACK_API_TOKEN=&quot;xoxp-945736515923-959414305302. . . . &quot;CHANNEL_ID=&quot;CU. . . &quot;SLACK_BOT_TOKEN=&quot;xoxb-945736515923-957048941172. . . . . &quot;SLACK_SIGNING_SECRET=&quot;88a3424d. . . . . &quot;ALPHA_VANTAGE_KEY=&quot;E1UR. . . . &quot;    Using Google Colab If you are running the notebook using Google Colab, remember to mount the directory that you are working from.       %%capturefrom google. colab import drivedrive. mount(&#39;/content/gdrive/&#39;)%cd /content/gdrive/My Drive/Colab Notebooks/ssh_files/smartie_slackbotimport syssys. path. append(&quot;. &quot;)  Go to this URL in a browser: https://accounts. google. com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i. apps. googleusercontent. com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2. 0%3aoob&amp;response_type=code&amp;scope=email%20https%3a%2f%2fwww. googleapis. com%2fauth%2fdocs. test%20https%3a%2f%2fwww. googleapis. com%2fauth%2fdrive%20https%3a%2f%2fwww. googleapis. com%2fauth%2fdrive. photos. readonly%20https%3a%2f%2fwww. googleapis. com%2fauth%2fpeopleapi. readonlyEnter your authorization code:··········        from credentials import SLACK_API_TOKEN, CHANNEL_ID, SLACK_BOT_TOKEN, \            SLACK_SIGNING_SECRET, ALPHA_VANTAGE_KEY    Requirements Not to forget, we need to install the dependencies. . .       %%capture!pip install alpha-vantage==2. 1. 3!pip install beautiful soup4==4. 8. 2!pip install slack==0. 0. 2!pip install slackclient==2. 5. 0!pip install slackeventsapi==2. 1. 0!pip install websocket-client==0. 57. 0    . . . and import the python packages that we will be using       import requestsimport timeimport pandas as pdimport numpy as npimport slackimport ssl as ssl_libimport certifiimport datetimefrom bs4 import BeautifulSoupfrom alpha_vantage. timeseries import TimeSeriesfrom pandas. tseries. holiday import USFederalHolidayCalendar as calendarfrom flask import Flaskfrom slackeventsapi import SlackEventAdapter    Retrieving the S&amp;P500 component stocks from wikipedia To get a list of S&amp;P500 component stocks, we will scrape a table from a wikipedia webpage. Whenever you require an updated list, you can revisit this function.       def get_sp500_stocks_wiki(url=None):  &quot;&quot;&quot;  Scapes a wikipedia web page to retrieve a table of S&amp;P500 component stocks.   Use this function if you require an updated table.   args:  ------    url: (str) url leading to the wikipedia web page  Return:  ------    df: (pd. DataFrame) a copy of the wikipedia table   &quot;&quot;&quot;  website_url = requests. get(url)  soup = BeautifulSoup(website_url. text, &#39;lxml&#39;)  my_table = soup. find(&#39;table&#39;, {&#39;class&#39;: &#39;wikitable sortable&#39;})  my_table  table_rows = my_table. find_all(&#39;tr&#39;)  data = []  for row in table_rows:    data. append([t. text. strip() for t in row. find_all(&#39;td&#39;)])  df = pd. DataFrame(data[1:], columns=[&#39;Ticker&#39;, &#39;Security&#39;, &#39;SEC_Filings&#39;,                     &#39;GICS&#39;, &#39;GICS_Sub&#39;, &#39;HQ&#39;,                     &#39;Date_First_Added&#39;, &#39;CIK&#39;, &#39;Founded&#39;])  return df    Although we have scrapped the whole table, we are mainly interested in the tickers, security name and GICS classification.       sp500_stocks_df = get_sp500_stocks_wiki(&quot;https://en. wikipedia. org/wiki/List_of_S%26P_500_companies&quot;)sp500_stocks_df. to_csv(&#39;. /data/sp500_stocks. csv&#39;, index=False)sp500_stocks_df[[&#39;Ticker&#39;, &#39;Security&#39;, &#39;GICS&#39; ]]. head(5)           Ticker   Security   GICS         0   MMM   3M Company   Industrials       1   ABT   Abbott Laboratories   Health Care       2   ABBV   AbbVie Inc.    Health Care       3   ABMD   ABIOMED Inc   Health Care       4   ACN   Accenture plc   Information Technology     Let's start coding our Slack bot! I have named my Slack bot as Smartie, and I have a created a class with a similar name. The class serves three main purposes: To retrieve the RSI of the stocks that we are interested in get_stocks_rsi. To convert the RSI readings into strings that can be read as texts on the Slack app get_rsi_string. To package the strings above into Slack compatible blocks and send them to our Slack channel with the relevant meta information get_message_payload_stock, get_message_payload. Fret not, if the class looks overwhelming! We will go through each of the methods so that you are able amend this class for your own use case.       class Smartie:  &quot;&quot;&quot;Slack app that sends stock analysis results for the day. &quot;&quot;&quot;  def __init__(self, channel):    self. channel = channel    self. username = &quot;smartie&quot;    self. reaction_task_completed = False    self. pin_task_completed = False    self. DIVIDER_BLOCK = {&quot;type&quot;: &quot;divider&quot;}  def get_stocks_rsi(self, rsi_n=14, stocks_n=100,            file_path=&#39;. /data/sp500_stocks. csv&#39;,            ind_excld=[&#39;Health Care&#39;, &#39;Utilities&#39;, &#39;Energy&#39;]):    &quot;&quot;&quot;    Calculates the Relative Strength Index (RSI) for a group of stocks    args:    ------      rsi_n: (list) size of rsi look-back period      stocks_n: (int) number of stocks to retrieve the rsi for      file_path (str): path leading to csv file of S&amp;P500 component                stocks      ind_excld: (list) GICS Sector industries to be excluded    Return:    ------      df_rsi: (pd. DataFrame) returns the rsi reading for the list of           stocks and whether the 30/70 levels have been breached    &quot;&quot;&quot;    sp500_stocks_df = get_sp500_stocks_file(file_path=file_path)    sp500_stocks_df_excld = filter_stocks_industry(sp500_stocks_df,                            ind_excld=ind_excld)    sp500_stocks_df_excld = sp500_stocks_df_excld. head(stocks_n)    info, symbols = get_stock_price(sp500_stocks_df_excld)    sp500_stocks_price_df = get_stock_price_df(info, symbols)    symbols = sp500_stocks_price_df[&#39;Symbol&#39;]. unique()    rsi_l = []    status_l = []    for s in symbols:      s_df = sp500_stocks_price_df[sp500_stocks_price_df[&#39;Symbol&#39;] == s]      closep = np. array(s_df[&#39;Close&#39;]. tolist())      closep = closep. astype(np. float)      rsi = get_rsi(closep, n=rsi_n)      if rsi[-1] &gt;=70:        status = &#39;Above 70&#39;      elif rsi[-1] &lt;= 30:        status = &#39;Below 30&#39;      else:        status = &#39;Normal&#39;      rsi_l. append(round(rsi[-1], 1))      status_l. append(status)    df_rsi = pd. DataFrame(zip(symbols, rsi_l, status_l),               columns=[&#39;Symbols&#39;, &#39;RSI&#39;, &#39;Status&#39;])    return df_rsi  def get_rsi_string(self, df_rsi, head_n=20, tail_n=20):    &quot;&quot;&quot;    Converts the RSI readings into strings to be displayed in Slack    args:    ------      df_rsi: (pd. DataFrame) the rsi readings for a list of           stocks and whether the 30/70 levels have been breached      head_n: (int) number of top ranked stocks (based on rsi) reading          to be displayed in Slack      tail_n: (int) number of bottom ranked stocks (based on rsi) reading           to be displayed in Slack    Return:    ------      top_str: (str) concatenated string for top ranked stocks&#39; rsi           reading      btm_str: (str) concatenated string for bottom ranked stocks&#39; rsi           reading    &quot;&quot;&quot;    df_rsi = df_rsi. sort_values(&#39;RSI&#39;,ascending=False)    df_top = df_rsi. head(head_n)    df_btm = df_rsi. tail(tail_n)    top_symbols = df_top[&#39;Symbols&#39;]. tolist()    btm_symbols = df_btm[&#39;Symbols&#39;]. tolist()    top_rsi = df_top[&#39;RSI&#39;]. tolist()    btm_rsi = df_btm[&#39;RSI&#39;]. tolist()    top_l = list(zip(top_symbols, top_rsi))    btm_l = list(zip(btm_symbols, btm_rsi))    top_str = &quot;&quot;    for el in top_l:      top_str = top_str + str(el[0]) + &quot; &quot; + str(el[1]) + &quot;\n&quot;    btm_str = &quot;&quot;    for el in btm_l:      btm_str = btm_str + str(el[0]) + &quot; &quot; + str(el[1]) + &quot;\n&quot;    return top_str, btm_str  def _get_text_block(self, string):    &quot;&quot;&quot;    Helper function for get_message_payload_stock method.     Used to convert a string into a slack formatted text block.     args:    ------      string: (str) concatenated string for the top or bottom ranked          stocks&#39; rsi reading    Return:    ------      dictionary containing the correctly formatted text block to be       displayed in slack    &quot;&quot;&quot;    return {&quot;type&quot;: &quot;section&quot;, &quot;text&quot;: {&quot;type&quot;: &quot;mrkdwn&quot;, &quot;text&quot;: string}}  def get_message_payload_stock(self, top_str, btm_str):    &quot;&quot;&quot;    Used to create a message payload to send stock rsi readings    args:    ------      top_str: (str) concatenated string for the top ranked stocks&#39;           rsi readings      bottom_str: (str) concatenated string for the bottom ranked            stocks&#39; rsi readings    Return:    ------      dictionary containing payload (stock rsi readings) to be sent      using Slack&#39;s API    &quot;&quot;&quot;    return {      &quot;channel&quot;: self. channel,      &quot;username&quot;: self. username,      &quot;blocks&quot;: [        self. _get_text_block(top_str),        self. DIVIDER_BLOCK,        self. _get_text_block(btm_str)      ],    }  def get_message_payload(self, string):    &quot;&quot;&quot;    Used to create a message payload to send simple text messages    args:    ------      string: (str) string to be sent using Slack&#39;s API    Return:    ------      dictionary containing payload (simple text messages) to be sent      using Slack&#39;s API.     &quot;&quot;&quot;    return {      &quot;channel&quot;: self. channel,      &quot;username&quot;: self. username,      &quot;text&quot;: string    }    Retrieving the RSI readings - overbought or oversold? : RSI was developed as a momentum oscillator that measures the speed and change of price movements. The index mainly oscillates between 0 to 100 and a stock is considered as overbought when it is above 70 and oversold when below 30. This is a relatively simple indicator and you can always substitute RSI for other indicators. Our aim with get_stocks_rsi is to retrieve a dataframe with stock names, their RSI readings and whether they are within the overbought (&gt;70), oversold (&lt;30), normal ranges. I have illustrated the output of this function using 10 stocks selected from a pool of companies.       from src. stock import get_sp500_stocks_wiki, get_sp500_stocks_file, \           filter_stocks_industry, get_stock_price, \           get_stock_price_df, get_rsi          def get_stocks_rsi(rsi_n=14, stocks_n=100, file_path=&#39;. /data/sp500_stocks. csv&#39;,          ind_excld=[&#39;Health Care&#39;, &#39;Utilities&#39;, &#39;Energy&#39;]):  &quot;&quot;&quot;  Calculates the Relative Strength Index (RSI) for a group of stocks  args:  ------    rsi_n: (list) size of rsi look-back period    stocks_n: (int) number of stocks to retrieve the rsi for    file_path (str): path leading to csv file of S&amp;P500 component              stocks    ind_excld: (list) GICS Sector industries to be excluded  Return:  ------    df_rsi: (pd. DataFrame) returns the rsi reading for the list of         stocks and whether the 30/70 level has been breached  &quot;&quot;&quot;  sp500_stocks_df = get_sp500_stocks_file(file_path=file_path)  sp500_stocks_df_excld = filter_stocks_industry(sp500_stocks_df,                          ind_excld=ind_excld)    sp500_stocks_df_excld = sp500_stocks_df_excld. head(stocks_n)  info, symbols = get_stock_price(sp500_stocks_df_excld)  sp500_stocks_price_df = get_stock_price_df(info, symbols)  symbols = sp500_stocks_price_df[&#39;Symbol&#39;]. unique()  rsi_l = []  status_l = []  for s in symbols:    s_df = sp500_stocks_price_df[sp500_stocks_price_df[&#39;Symbol&#39;] == s]    closep = np. array(s_df[&#39;Close&#39;]. tolist())    closep = closep. astype(np. float)    rsi = get_rsi(closep, n=rsi_n)    if rsi[-1] &gt;=70:      status = &#39;Above 70&#39;    elif rsi[-1] &lt;= 30:      status = &#39;Below 30&#39;    else:      status = &#39;Normal&#39;    rsi_l. append(round(rsi[-1], 1))    status_l. append(status)  df_rsi = pd. DataFrame(zip(symbols, rsi_l, status_l),             columns=[&#39;Symbols&#39;, &#39;RSI&#39;, &#39;Status&#39;])  return df_rsi    For this example, there is only one stock (AMD) that crossed the 30/70 threshold.       df_rsi = get_stocks_rsi(rsi_n=14, stocks_n=10,             file_path=&#39;. /data/sp500_stocks. csv&#39;,             ind_excld=[&#39;Health Care&#39;, &#39;Utilities&#39;, &#39;Energy&#39;])df_rsi. head(10)           Symbols   RSI   Status         0   MMM   49. 6   Normal       1   ACN   53. 1   Normal       2   ATVI   66. 0   Normal       3   ADBE   39. 4   Normal       4   AMD   29. 1   Below 30       5   AAP   44. 5   Normal       6   AFL   35. 0   Normal       7   APD   42. 2   Normal       8   AKAM   67. 4   Normal       9   ALK   42. 9   Normal     Let's breakdown get_stocks_rsi. First we retrieve a dataframe of the S&amp;P500 component stocks. Thankfully we have already saved it in a csv file earlier and all we need to do is to read the file.       def get_sp500_stocks_file(file_path=None):  &quot;&quot;&quot;  Reads a csv file containing the wikipedia S&amp;P500 component stocks table  args:  ------    file_path: (str) path leading to the csv file  Return:  ------    df: (pd. DataFrame) a copy of the wikipedia table  &quot;&quot;&quot;  df = pd. read_csv(file_path)  return df          sp500_stocks_df = get_sp500_stocks_file(file_path=&quot;. /data/sp500_stocks. csv&quot;)sp500_stocks_df[[&#39;Ticker&#39;, &#39;Security&#39;, &#39;GICS&#39; ]]. head(10)           Ticker   Security   GICS         0   MMM   3M Company   Industrials       1   ABT   Abbott Laboratories   Health Care       2   ABBV   AbbVie Inc.    Health Care       3   ABMD   ABIOMED Inc   Health Care       4   ACN   Accenture plc   Information Technology       5   ATVI   Activision Blizzard   Communication Services       6   ADBE   Adobe Inc.    Information Technology       7   AMD   Advanced Micro Devices Inc   Information Technology       8   AAP   Advance Auto Parts   Consumer Discretionary       9   AES   AES Corp   Utilities     You can exclude any industries that you are not interested in. In this scenario, I have excluded companies from the Health Care, Utilities and Energy industries.       def filter_stocks_industry(df, ind_excld=[]):  &quot;&quot;&quot;  Filters a dataframe based on the GICS industries  args:  ------    df: (pd. DataFrame) a copy of the wikipedia table    ind_excld: (list) GICS industries to be excluded  Return:  ------    df_excld: (pd. DataFrame) filtered dataframe  &quot;&quot;&quot;  df_excld = df[~df[&#39;GICS&#39;]. isin(ind_excld)]  return df_excld          sp500_stocks_df_excld = filter_stocks_industry(sp500_stocks_df,                        ind_excld=[&#39;Health Care&#39;,                             &#39;Utilities&#39;,                             &#39;Energy&#39;])sp500_stocks_df_excld[[&#39;Ticker&#39;, &#39;Security&#39;, &#39;GICS&#39; ]]. head(10)           Ticker   Security   GICS         0   MMM   3M Company   Industrials       4   ACN   Accenture plc   Information Technology       5   ATVI   Activision Blizzard   Communication Services       6   ADBE   Adobe Inc.    Information Technology       7   AMD   Advanced Micro Devices Inc   Information Technology       8   AAP   Advance Auto Parts   Consumer Discretionary       10   AFL   AFLAC Inc   Financials       12   APD   Air Products &amp; Chemicals Inc   Materials       13   AKAM   Akamai Technologies Inc   Information Technology       14   ALK   Alaska Air Group Inc   Industrials     Now that we have the stocks that we would like to focus on, it is time for us to get the historical stock price for each of them. We will use the first 10 stocks of our dataframe for illustration. As Alpha Vantage limits the number of API calls per minute (5 calls/minute), we will tell the program to retrieve the stock information at intervals of 65 seconds.       def get_stock_price(df_excld):  &quot;&quot;&quot;  Retrieves the daily stock price from a dataframe of stocks and their  respective tickers  args:  ------    df_excld: (pd. DataFrame) filtered dataframe of stocks containing only         stocks from selected industries  Return:  ------    info: (list) a complete history of stocks&#39; pricing/volume information     symbols: (list) stock tickers  &quot;&quot;&quot;  ts = TimeSeries(ALPHA_VANTAGE_KEY)  info = []  symbols = []  counter = 0  for t in df_excld[&#39;Ticker&#39;]:    if counter % 5 == 0:      time. sleep(65)    i, m = ts. get_daily(symbol=t, outputsize=&#39;full&#39;)    info. append(i)    symbols. append(m[&#39;2. Symbol&#39;])    counter += 1  return info, symbols          sp500_stocks_df_excld = sp500_stocks_df_excld. head(10)info, symbols = get_stock_price(sp500_stocks_df_excld)    We have managed to obtain the price and volume information for our stocks. Let's zoom in and look at MMM's (first ticker) pricing and volume information.       info[0][&#39;2020-02-28&#39;]  {&#39;1. open&#39;: &#39;154. 0900&#39;, &#39;2. high&#39;: &#39;156. 7200&#39;, &#39;3. low&#39;: &#39;146. 0000&#39;, &#39;4. close&#39;: &#39;149. 2400&#39;, &#39;5. volume&#39;: &#39;11498521&#39;}  And these are the stocks that we have extracted the information for.       symbols  [&#39;MMM&#39;, &#39;ACN&#39;, &#39;ATVI&#39;, &#39;ADBE&#39;, &#39;AMD&#39;, &#39;AAP&#39;, &#39;AFL&#39;, &#39;APD&#39;, &#39;AKAM&#39;, &#39;ALK&#39;]  Now that we have retrieved all the information we need, let's convert it into a dataframe for better visualization.       def get_stock_price_df(info, symbols):  &quot;&quot;&quot;  Converts pricing/volume information and the stocks symbols into  a dataframe  args:  ------    info: (list) a complete history of stocks&#39; pricing/volume information     symbols: (list) stock tickers  Return:  ------    df_full: (pd. DataFrame) consists of stock tickers their pricing/volume         information  &quot;&quot;&quot;  df_l = []  for num, i in enumerate(info):    df = pd. DataFrame. from_dict(i, orient=&#39;index&#39;)    df[&#39;Symbol&#39;] = symbols[num]    df_l. append(df)  df_full = pd. concat(df_l)  df_full = df_full. rename(columns={&#39;1. open&#39;: &#39;Open&#39;,                   &#39;2. high&#39;: &#39;High&#39;,                   &#39;3. low&#39;: &#39;Low&#39;,                   &#39;4. close&#39;: &#39;Close&#39;,                   &#39;5. volume&#39;: &#39;Volume&#39;})  return df_full          sp500_stocks_price_df = get_stock_price_df(info, symbols)sp500_stocks_price_df. head(5)           Open   High   Low   Close   Volume   Symbol         2020-03-03   152. 4400   154. 0000   144. 4400   145. 2400   8253737   MMM       2020-03-02   151. 3400   153. 4300   148. 3700   153. 0200   8021045   MMM       2020-02-28   154. 0900   156. 7200   146. 0000   149. 2400   11498521   MMM       2020-02-27   151. 2300   155. 4300   149. 0000   150. 1600   8217960   MMM       2020-02-26   149. 5700   151. 8200   148. 0900   148. 9600   5151670   MMM     Finally, we can feed the stock prices into get_rsi to obtain the RSI readings.       def get_rsi(prices, n=14):  &quot;&quot;&quot;  Calculates the Relative Strength Index (RSI) for a stock  Credits: sentdex - https://www. youtube. com/watch?v=4gGztYfp3ck  args:  ------    prices: (list) prices for a stock    n: (int) size of RSI look-back period  Return:  ------    rsi: (float): momentum indicator that measures the magnitude of recent           price changes to evaluate overbought or oversold            conditions    https://www. investopedia. com/terms/r/rsi. asp  &quot;&quot;&quot;  deltas = np. diff(prices)  seed = deltas[:n+1]  up = seed[seed &gt;= 0]. sum()/n  down = -seed[seed &lt; 0]. sum()/n  rs = up/down  rsi = np. zeros_like(prices)  rsi[:n] = 100. - 100. /(1. + rs)  for i in range(n, len(prices)):    delta = deltas[i-1]    if delta &gt; 0:      upval = delta      downval = 0.     else:      upval = 0.       downval = -delta    up = (up*(n-1) + upval)/n    down = (down*(n-1) + downval)/n    rs = up/down    rsi[i] = 100. - 100. /(1. +rs)  return rsi    Converting our RSI readings from a dataframe to concatenated strings : Unfortunately, as much as we want to, we are not able to send the whole dataframe we have created above as a message to our Slack channel. One workaround is for us to first extract the stock tickers and RSI readings as strings. After that, we will concatenate the strings together before sending them to our Slack channel.        def get_rsi_string(df_rsi, head_n=20, tail_n=20):  &quot;&quot;&quot;  Converts the RSI readings into strings to be displayed in Slack  args:  ------    df_rsi: (pd. DataFrame) the rsi readings for a list of         stocks and whether the 30/70 levels have been breached    head_n: (int) number of top ranked stocks (based on rsi) reading        to be displayed in Slack    tail_n: (int) number of bottom ranked stocks (based on rsi) reading         to be displayed in Slack  Return:  ------    top_str: (str) concatenated string for top ranked stocks&#39; rsi         reading    btm_str: (str) concatenated string for bottom ranked stocks&#39; rsi         reading  &quot;&quot;&quot;  df_rsi = df_rsi. sort_values(&#39;RSI&#39;, ascending=False)  df_top = df_rsi. head(head_n)  df_btm = df_rsi. tail(tail_n)  top_symbols = df_top[&#39;Symbols&#39;]. tolist()  btm_symbols = df_btm[&#39;Symbols&#39;]. tolist()  top_rsi = df_top[&#39;RSI&#39;]. tolist()  btm_rsi = df_btm[&#39;RSI&#39;]. tolist()  top_l = list(zip(top_symbols, top_rsi))  btm_l = list(zip(btm_symbols, btm_rsi))  top_str = &quot;&quot;  for el in top_l:    top_str = top_str + str(el[0]) + &quot; &quot; + str(el[1]) + &quot;\n&quot;  btm_str = &quot;&quot;  for el in btm_l:    btm_str = btm_str + str(el[0]) + &quot; &quot; + str(el[1]) + &quot;\n&quot;  return top_str, btm_str          top_str, btm_str = get_rsi_string(df_rsi, head_n=5, tail_n=5)    This is the concatenated string for the top 5/ bottom 5 ranked stocks (based on RSI readings). The escape sequence  \n  will help to display each stock on a new line when viewed on the Slack channel.       top_str  &#39;AKAM 67. 4\nATVI 66. 0\nACN 53. 1\nMMM 49. 6\nAAP 44. 5\n&#39;        btm_str  &#39;ALK 42. 9\nAPD 42. 2\nADBE 39. 4\nAFL 35. 0\nAMD 29. 1\n&#39;  We're almost there! Let's integrate our code with Slack's API : For us to send the concatenated string to our Slack Channel, Slack requires us to format our strings in the form of a  block  that contains other required meta information. We will be using the _get_text_block helper function to help us do just that.       def _get_text_block(self, string):  &quot;&quot;&quot;  Helper function for get_message_payload_stock.   Used to convert a string into a slack formatted text block.   args:  ------    string: (str) concatenated string for the top or bottom ranked         stocks&#39; rsi reading  Return:  ------    dictionary containing the correctly formatted text block to be    displayed in slack  &quot;&quot;&quot;  return {&quot;type&quot;: &quot;section&quot;, &quot;text&quot;: {&quot;type&quot;: &quot;mrkdwn&quot;, &quot;text&quot;: string}}    We can then piece together our  text_block  with other user specific information to create our payload. Additional information that are required: channel: ID of the channel that we are sending this message tousername: The name of our slackbotWe will use the following two methods to send two types of payload: get_message_payload_stock: used to send our RSI readings to the Slack channel. This payload includes a divider block that separates the top and bottom ranked RSI readings. get_message_paylaod: used to send simple texts.       def get_message_payload_stock(self, top_str, btm_str):  &quot;&quot;&quot;  Used to create a message payload to send stock rsi readings  args:  ------    top_str: (str) concatenated string for the top ranked stocks&#39;         rsi readings    bottom_str: (str) concatenated string for the bottom ranked stocks&#39;          rsi readings  Return:  ------    dictionary containing payload (stock rsi readings) to be sent using    Slack&#39;s API  &quot;&quot;&quot;  return {    &quot;channel&quot;: self. channel,    &quot;username&quot;: self. username,    &quot;blocks&quot;: [      self. _get_text_block(top_str),      self. DIVIDER_BLOCK,      self. _get_text_block(btm_str)    ],  }          def get_message_payload(self, string):  &quot;&quot;&quot;  Used to create a message payload to send simple text messages  args:  ------    string: (str) string to be sent using Slack&#39;s API  Return:  ------    dictionary containing payload (simple text messages) to be sent    using Slack&#39;s API  &quot;&quot;&quot;  return {    &quot;channel&quot;: self. channel,    &quot;username&quot;: self. username,    &quot;text&quot;: string  }    Wait a minute. . . : Before I demonstrate how we can use the methods to send our payloads to Slack, I would like to introduce get_fed_holidays which will help us retrieve the days that the US market is closed. When the market is closed, we are not able to retrieve new stock information, hence there is no need for us to retrieve new RSI readings. During periods like this, we can give our bot a break and instead notify us with a message 'Market closed, US Federal holiday'.       def get_fed_holidays(start_date, end_date):  &quot;&quot;&quot;  Retrieve a dataframe outlining the days that the US market is closed  args:  ------    start_date: (str) start date for the period in focus    end_date: (str) end date for the period in focus  Return:  ------    df_holiday: (pd. DataFrame) returns the days that are US market holidays  &quot;&quot;&quot;  dr = pd. date_range(start=start_date, end=end_date)  df = pd. DataFrame()  df[&#39;Date&#39;] = dr  cal = calendar()  holidays = cal. holidays(start=dr. min(), end=dr. max())  df[&#39;Holiday&#39;] = df[&#39;Date&#39;]. isin(holidays)  df_holiday = df[df[&#39;Holiday&#39;] == True]  return df_holiday    Ok now let us see how we can send simple text messages and our RSI readings sent to Slack. Let's first instantiate the class with our channel ID       s = Smartie(CHANNEL_ID)    If it is a US Federal holiday, the following payload will be sent to the Slack channel. . . .       message = s. get_message_payload(&#39;Market closed, US Federal holiday&#39;)message  {&#39;channel&#39;: &#39;CU560GVCG&#39;, &#39;text&#39;: &#39;Market closed, US Federal holiday&#39;, &#39;username&#39;: &#39;smartie&#39;}  . . . else we will send the RSI readings.       message = s. get_message_payload_stock(top_str, btm_str)message  {&#39;blocks&#39;: [{&#39;text&#39;: {&#39;text&#39;: &#39;AKAM 67. 4\nATVI 66. 0\nACN 53. 1\nMMM 49. 6\nAAP 44. 5\n&#39;,  &#39;type&#39;: &#39;mrkdwn&#39;},  &#39;type&#39;: &#39;section&#39;}, {&#39;type&#39;: &#39;divider&#39;}, {&#39;text&#39;: {&#39;text&#39;: &#39;ALK 42. 9\nAPD 42. 2\nADBE 39. 4\nAFL 35. 0\nAMD 29. 1\n&#39;,  &#39;type&#39;: &#39;mrkdwn&#39;},  &#39;type&#39;: &#39;section&#39;}], &#39;channel&#39;: &#39;CU560GVCG&#39;, &#39;username&#39;: &#39;smartie&#39;}  Sending our first message! Let's put everything we have worked on so far and see how the messages looks like on our Slack channel!       import slackslack_web_client = slack. WebClient(token=SLACK_BOT_TOKEN)          response = slack_web_client. chat_postMessage(**message)response  &lt;slack. web. slack_response. SlackResponse at 0x7f70e6eb45c0&gt;        import logginglogger = logging. getLogger()logger. setLevel(logging. DEBUG)s = Smartie(CHANNEL_ID)logger. info(&#39;Getting fed holidays&#39;)range_start_date = &#39;2020-01-01&#39;range_end_date = &#39;2020-12-31&#39;fed_holiday = get_fed_holidays(start_date=range_start_date,                end_date=range_end_date)if str(datetime. datetime. now(). date()) in fed_holiday:  logger. info(&#39;It is a Holiday&#39;)  message = s. get_message_payload(&#39;Market closed, US Federal holiday&#39;)else:  logger. info(&#39;It is not a Holiday&#39;)  df_rsi = s. get_stocks_rsi(rsi_n=14, stocks_n=10)  top_str, btm_str = s. get_rsi_string(df_rsi, head_n=5, tail_n=5)  message = s. get_message_payload_stock(top_str, btm_str)logger. info(&#39;Posting on Slack&#39;)response = slack_web_client. chat_postMessage(**message)    Great! You should have received a message like the one below in your channel (that is if isn't a US Federal holiday).  Deployment Dockerfile : The Dockerfile outlines a set of instructions/commands needed to build a docker image. This can include simple commands such as COPY which adds files from a source on your computer into the Docker image. We will use the Miniconda base image which comes with Python.       # our base imageFROM continuumio/miniconda3# update essential packagesRUN apt update &amp;&amp; \  apt -y install bzip2 curl gcc ssh rsync git vim cron &amp;&amp; \  apt-get clean# creating our conda environmentARG CONDA_ENV_FILEARG CONDA_PATH=&quot;/root/miniconda3/bin&quot;ARG CONDA_BIN=&quot;$CONDA_PATH/conda&quot;COPY $CONDA_ENV_FILE $CONDA_ENV_FILEENV SHELL /bin/bashRUN conda install nb_condaENV PATH /root/miniconda3/envs/stock_screener/bin:$CONDA_PATH:$PATH# update the environmentCOPY conda. yml . RUN conda env update -n stock_screener --file . /conda. yml# adding our environment variables for Cron job to workRUN printenv &gt;&gt; /etc/environment# allow log messages to be printed in interactive terminalENV PYTHONUNBUFFERED 1# run shell scriptRUN chmod +x . /run. shENTRYPOINT [&quot;. /run. sh&quot;]    Conda. yml : The conda. yml file will be used to upgrade the python version and install any package dependencies that we need in our conda environment.       channels: - defaultsdependencies: - python=3. 6. 7 - pip=19. 0. 3 - nb_conda - pip:  - alpha-vantage==2. 1. 3  - beautifulsoup4==4. 8. 2  - pandas==1. 0. 1  - numpy==1. 18. 1  - slack==0. 0. 2  - slackclient==2. 5. 0  - slackeventsapi==2. 1. 0  - websocket-client==0. 57. 0    run. sh : For our log messages to be printed out in the interactive terminal, we will need to create a softlink /var/log/cron. log to /proc/1/fd/1. This will allow standard output and error streams that are passed to the softlink /var/log/cron. log to be directed to /proc/1/fd/1 which can then be viewed in the interactive terminal. As we would like the app to send us updates on a periodic basis, we will have to tell the server to run our code repeatedly. To do this, we can use the Cron service which is available on our server by listing down the things we need the Cron service to work on in the /etc/crontab file. * denotes the schedule that the code will run at. As an example, 0 1 * * * will mean that the code will run every day at 1am. Currently, I have scheduled the script to run every 5 minutes for testing */5 * * * *.        #!/bin/bashprintenv &gt; /etc/default/localeservice cron startecho &#39;Smartie is ready for action!&#39;ln -sf /proc/1/fd/1 /var/log/cron. logecho &#39;*/5 * * * * root cd / &amp;&amp; /opt/conda/envs/stock_screener/bin/python . /app. py &gt;&gt; /var/log/cron. log&#39; &gt; /etc/crontabtail -f /var/log/cron. log    env. list (you will need to create this) : Remember the credentials that we painstakingly created and saved in credentials. py? You will need to create a env. list file and copy all your credentials over. Just that this time, the double quotes    can be omitted from the env. list file. Save the file on your local computer. You should have something similar in env. list.       SLACK_API_TOKEN=xoxp-945736515923-959414305302. . . . CHANNEL_ID=CU. . . SLACK_BOT_TOKEN=xoxb-945736515923-957048941172. . . . . SLACK_SIGNING_SECRET=88a3424d. . . . . ALPHA_VANTAGE_KEY=E1UR. . . .     Creating the docker image on docker hub : First let's build the Docker image from the Dockerfile. You will need to run the following code in your terminal, in the same directory as the Dockerfile. Replace  kennethwang92  with your Docker ID. docker build -t kennethwang92/stock_screener After building, you can tag it with a name eg. demo and push the image to docker hub. Replace 0e5574283393 with your image ID. By default your image would have been tagged with  latest , you can substitute it with another label if you prefer. In this example, let's label the image as demo. docker tag 0e5574283393 kennethwang92/stock_screener:demo Login to docker hub on your terminal. Key in your username and password when prompted. docker login docker. io Push the image to docker hub. docker push kennethwang92/stock_screener:demo Deploying on a Digital Ocean server (droplet) Now that we have pushed the image to Docker hub, we can create a server using Digital Ocean for us to deploy the container. Below is a step-by-step guide on how to create a server (Digital Ocean calls it a droplet). Creating a server : Set up an account with Digital Ocean here.  Create a project.  Create a droplet.  Select ubuntu.  Choose a plan.  Select a datacenter.  Create ssh. Click on  New SSH Key  and follow the instructions.  Click on create droplet.  Select droplet.  Save the IP address. We will need to use it in the next section.  Connecting to the server and building the docker container : First let's copy the env. list file that you saved in your local computer into the server. You will have to amend ~/. ssh/id_rsa and . /env. list depending on where you have saved your ssh keys and env. list file. Insert the [ip address] that you have saved in the earlier section. scp -r -i ~/. ssh/id_rsa . /env. list root@[ip address]:~ Now we will connect to the server. ssh -i ~/. ssh/id_rsa root@[ip address] You should see on your terminal that you have connected to the server.  While in the server, type the following commands in the following order to: Update apt-get command-line tool for handling packagesapt-get update Install dockerapt install docker. io Download the image from Docker Hubdocker pull kennethwang92/stock_screener: demo Run the image with our credentials in env. list to create our docker container. Replace . /env. list with the path leading to your env. list file. For my case I have saved the file on my desktop. Replace  kennethwang92  with your Docker ID if you would like to use your own image. Ensure that you are running this code in the same directory as the env. list file. docker run -it --env-file . /env. list kennethwang92/stock_screener: demo /bin/bash And we're done! Check your Slack channel for the messages! Future improvements: Slack bots are really powerful and when used with python, the possibilities are endless. Here are some improvements I can think of: Instead of using RSI, you can use other indicators that you are interested in. You can even execute trades through the bot if your broker has an API that you can connect to. Send . png/. jpg graphs instead of texts.  I hope you enjoyed this article and you find the Slack bot useful. If you have any questions/comments please feel free to reach out to me through Linkedin or email at ken. wangtm@gmail. com "
    }, {
    "id": 5,
    "url": "https://kenneth-wang.github.io/2020/03/06/_02_27_Geopolitical_Mapper.html",
    "title": "Mapping the world",
    "body": "2020/03/06 -           About It feels like the stock market tend to move in tandem with the political headlines. Benjamin Graham, the father of value investing once quoted  In the short run, the market is a voting machine but in the long run, it is a weighting machine.   I'm sure you have witnessed headlines such as  Dow ends down 2. 4% on escalating US-China trade war  before. Being able to sift through the noise in the headlines could make the media an unexpected ally for investing. In response to demand for such forms of alternative data, Bloomberg introduced a tool to generate sentiments from news and social media data. In this blog post, we'll walk through the steps to analyze text data and generate sentiments out of the relationships between countries by using ALBERT. What is ALBERT? Ok what is ALBERT exactly? ALBERT stands for  A Lite BERT For Self-Supervised Learning of Language Representations . ALBERT was introduced by Google as a newly improved version of BERT, or Bidirectional Encoder Representations with Transformers. It achieved state-of-the art results on three popular benchmark tests for nautral language understanding (GLUE, SQUAD and RACE). ALBERT improves on BERT by: utilizing factorized embedding parametrization. From a language modelling perspective, input-level embeddings are context-independent representations of the words (eg.  bank ), whereas the hidden-layer embeddings are meant to learn context-dependent representations (eg.  bank  in the context of financial transactions or river-flow management). Traditionally the sizes of the input-level and hidden layer embeddings are the same and pegged to one another. ALBERT takes a different approach by reducing the size of the input-level embeddings (128), while keeping hidden-layer embeddings at a higher dimension (768). With this, ALBERT managed to reduce the number of parameters drastically at the expense of a minor drop in performance. implementing cross-layer parameter sharing. The authors implemented cross-layer parameter (feed-forward network parameters and attention parameters) sharing and results show that weight-sharing has an effect on stabilizing network parameters. As shown below, the differences (L2/ Cosine Similarity measures) between the input and output embeddings for ALBERT do not fluctuate as vigorously when compared to BERT. making amendments to the Sentence Order Prediction training task. During the training phase, BERT receives two sentences each time and it needs to predict whether the second sentence in the pair, has been swapped with a sentence from another document. ALBERT implements a similar methodolgy, however it chooses instead to predict whether the order of two consecutive sentences in the same document have been swapped. The authors felt that the task of predicting the order of sentences is a more challenging pretraining task. Due to the design choices of the authors, the ALBERT models have a smaller set of parameters.  Project directory structure The files that we will be referring to can be found HERE. You can refer to the project structure below to find the locations of the files we need. We will be using SQUAD's SST-2 dataset to finetune the ALBERT model before using transfer learning to train on our own data. We will use Wikipedia's API to extract text data that outlines the relationships between countries.       geopolitical_mapper/_______ data/___________ SST-2/ # SST-2 dataset___________ Wiki/ # Wikipedia/ country-related data______________ countries. csv # list of countries in the world______________ country_ISO. txt # mapping of countries and their ISO code______________ wiki_relations. csv # wikipedia summaries of country relationships______________ dev. tsv # validation dataset ______________ train. tsv # training dataset______________ sent_l_. csv # labeled dataset (without labels)______________ sent_ld. csv # labeled dataset (with labels)______________ sent_ul. csv # unlabeled dataset_______ model/ # Finetuned ALBERT model weights and checkpoints_______ 2020-02-27-Geopolitical-Mapper. ipynb    Requirements Let's start coding! Let's install the packages we need and import them.              #collapse-hide%%capture!pip install bs4!pip install plotly!pip install requests!pip install sklearn!pip install spacy!pip install tensorflow==2. 0. 0!pip install tensorflow-gpu==2. 0. 0!pip install transformers==2. 3. 0!pip install wikipedia                    #collapse-hideimport csvimport itertoolsimport osimport reimport requestsimport spacyimport urllib. requestimport wikipediaimport zipfileimport matplotlib. pyplot as pltimport numpy as npimport pandas as pdimport tensorflow as tffrom bs4 import BeautifulSoupfrom sklearn. model_selection import train_test_split, GroupShuffleSplitfrom sklearn. metrics import precision_score, recall_score, f1_score, \              accuracy_score              from spacy. lang. en import Englishfrom transformers import AlbertTokenizer, glue_convert_examples_to_featuresfrom transformers. data. processors import utilsfrom transformers. modeling_tf_albert import TFAlbertForSequenceClassificationfrom tensorflow. keras. callbacks import ReduceLROnPlateau, ModelCheckpoint, \                    EarlyStopping       If you are running the notebook using Google Colab, remember to mount the directory that you are working from.              #collapse-hide%%capturefrom google. colab import drivedrive. mount(&#39;/content/gdrive/&#39;)%cd /content/gdrive/My Drive/Colab Notebooks/ssh_files/geopolitical_mapper     Go to this URL in a browser: https://accounts. google. com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i. apps. googleusercontent. com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2. 0%3aoob&amp;response_type=code&amp;scope=email%20https%3a%2f%2fwww. googleapis. com%2fauth%2fdocs. test%20https%3a%2f%2fwww. googleapis. com%2fauth%2fdrive%20https%3a%2f%2fwww. googleapis. com%2fauth%2fdrive. photos. readonly%20https%3a%2f%2fwww. googleapis. com%2fauth%2fpeopleapi. readonlyEnter your authorization code:··········  Get alternative names of countries One of the biggest problems we face is that each country can have multiple naming conventions. For example, depending on the data that you use,  United States  might be named as  United States of America  or  USA . Hence we need to look for a source that give us the different naming conventions for each of the countries. Thankfully we are able to source the names from a Wikipage.  Each of the tables in the Wikipage has two columns. The column on the left, lists down the  standardized names  of the countries in alphabetical order. . .       website_url = requests. get(&#39;https://en. wikipedia. org/wiki/List_of_alternative_country_names&#39;). textsoup = BeautifulSoup(website_url,&#39;lxml&#39;)tables = soup. find_all(&#39;table&#39;,{&#39;class&#39;:&#39;wikitable&#39;})countries = []## Left column - &quot;Description&quot;# Each table contains countries that start with a certain alphabetfor table in tables: rows = table. find_all(&#39;tr&#39;)  # Gather countries with the specific alphabet  countries_alphabet = [row. find(&#39;a&#39;). get(&#39;title&#39;) \            for i, row in enumerate(rows) if i != 0 ] for country in countries_alphabet:  countries. append(country)    . . . while the columns on the right lists down the alternative names of the countries.       countries_alt_names = []# Right column - Other name(s) or older name(s)for table in tables: rows = table. find_all(&#39;tr&#39;) for i, r in enumerate(rows):    # Exclude table headers  if i != 0:   country_alt_name = [name. text for name in r. findAll(&#39;b&#39;)]      if country_alt_name:    countries_alt_names. append(country_alt_name)    After scrapping the table from the Wikipage, we can create a dictionary alt_names_to_countries that maps each alternative name of the countries to their standardized name. (eg.  USA :  United States ). In the process of doing so we will: convert all letters to lower caseremove  the remove any empty spaces    . With these changes, names such as  The United States of America  will be modified to  united_states_of_america        # First we group each country with their respective alternative namescountries_to_alt_names_raw = {c:n for c, n in zip(countries, countries_alt_names)}# Map each alternative name to the default country namealt_names_to_countries = {v[i]:k for k, v in countries_to_alt_names_raw. items() \             for i in range(len(v))}# We lowercase the names; Remove &quot;the&quot; and &#39;&#39; that are contained in the namesalt_names_to_countries = {k. lower(). replace(&quot;the &quot;, &quot;&quot;): v. lower(). replace(&quot;the &quot;, &quot;&quot;) \             for k, v in alt_names_to_countries. items()}alt_names_to_countries = {k. lower(). replace(&quot; &quot;, &quot;_&quot;): v. lower(). replace(&quot; &quot;, &quot;_&quot;) \             for k, v in alt_names_to_countries. items()}    We remove names that are empty strings and add some names that were missed out by the Wikipage.       # remove empty stringsalt_names_to_countries. pop(&#39;&#39;, None)# Add namesalt_names_to_countries[&#39;angola&#39;] = &#39;angola&#39;alt_names_to_countries[&#39;equatorial_guinea&#39;] = &#39;equatorial_guinea&#39;    The mappings of standardized names with themselves are valid mappings as well. Let's add them in.       default_countries = list(set(alt_names_to_countries. values()))for c in default_countries: alt_names_to_countries. update({c:c})    Let's convert the dictionary alt_names_to_countries into a dataframe alt_names_df for easier visualization.       alt_names_df = pd. DataFrame. from_dict(alt_names_to_countries, \                   orient=&#39;index&#39;). reset_index()alt_names_df = alt_names_df. rename(columns={&#39;index&#39;: &#39;Alternative_Name&#39;, \                      0:&#39;Name&#39;})alt_names_df. head(10)           Alternative_Name   Name         0   republic_of_abkhazia   abkhazia       1   aphsny_axwynthkharra   abkhazia       2   respublika_abkhaziya   abkhazia       3   autonomous_republic_of_abkhazia   abkhazia       4   islamic_republic_of_afghanistan   afghanistan       5   da_afġānistān_islāmī_jumhoryat   afghanistan       6   jomhūrīyyeh_eslāmīyyeh_afġānestān   afghanistan       7   republic_of_albania   albania       8   republika_e_shqipërisë   albania       9   arnavutluk   albania     As it will be easier for us to work with country names when they are standardized, let's create a function get_std_names that converts the raw names to the standardized (std) names by using the mappings in alt_names_to_countries. Sometimes raw names that we deal with may be upper cased or contain words/characters such as  the , brackets and punctuations. We will clean them up before looking for their standardized names.       def get_std_names(df, col_name, alt_names_to_countries):  &quot;&quot;&quot;  Creates a new column of standardized country names for a dataframe    args:  ------    df: (pd. DataFrame) initial dataframe    col_name: (str) name of the column containing the raw country names             eg. &quot;Country_A&quot;    alt_names_to_countries: (dict) contains country mappings  Return:  ------    New dataframe with an additional column with the suffix &quot;_Std&quot;     representing the standardized country names eg. &quot;Country_A_Std&quot;  &quot;&quot;&quot;  country_names = df[col_name]. tolist()  # lower case  country_names = [c. lower() for c in country_names]  # remove the  country_names = [c. replace(&quot;the &quot;,&quot;&quot;) for c in country_names]  # add underscore  country_names = [&quot;_&quot;. join(c. split()) for c in country_names]  # remove &#39;(&#39; &#39;)&#39;  country_names = [c. replace(&quot;(&quot;, &quot;&quot;) for c in country_names]  country_names = [c. replace(&quot;)&quot;, &quot;&quot;) for c in country_names]  # remove .   country_names = [c. replace(&quot;. &quot;, &quot;&quot;) for c in country_names]    std_names = []  for name in country_names:   try:    std_names. append(alt_names_to_countries[name])   except:    std_names. append(name)  df[col_name + &#39;_Std&#39;] = std_names  return df    Great, now let's put the get_std_names function to good use! As you can see, we have an additional column Country_Std that contains the standardized names.       countries_df = pd. read_csv(&#39;. /data/Wiki/countries. csv&#39;)countries_df = get_std_names(countries_df, &#39;Country&#39;, alt_names_to_countries)countries_df. head(10)           Continent   Country   Country_Std         0   Africa   Algeria   algeria       1   Africa   Angola   angola       2   Africa   Benin   benin       3   Africa   Botswana   botswana       4   Africa   Burkina   burkina       5   Africa   Burundi   burundi       6   Africa   Cameroon   cameroon       7   Africa   Cape Verde   cape_verde       8   Africa   Central African Republic   central_african_republic       9   Africa   Chad   chad     With the names of the countries all accounted for, we will gather the total number of pair-wise relations between countries that we have to analyze the sentiments for. We will grab a list of unique countries that we have, group them up and remove any duplicate pairs. And bam! We have 18,336 country pairs that we have to work with.       country_groups = []unique_countries = countries_df[&#39;Country_Std&#39;]. tolist()for i in range(len(unique_countries)): for j in range(len(unique_countries)):  country_A = unique_countries[i]  country_B = unique_countries[j]  # We are not interested in relations of countries with itself  if country_A != country_B:   country_groups. append([country_A, country_B])# Sort by alphabetical ordersorted_country_groups = [tuple(sorted(el)) for el in country_groups]# Remove any duplicatesunique_country_groups = list(dict. fromkeys(sorted_country_groups))print(len(unique_country_groups))print(unique_country_groups[0:5])  18336[(&#39;algeria&#39;, &#39;angola&#39;), (&#39;algeria&#39;, &#39;benin&#39;), (&#39;algeria&#39;, &#39;botswana&#39;), (&#39;algeria&#39;, &#39;burkina&#39;), (&#39;algeria&#39;, &#39;burundi&#39;)]  Gathering the data from Wikipedia! To gather the data we need, one option will be to scrape news headlines from the web. But one of the problems that we face when doing a simple search with  China-United States relations  on Google is that you are unable to differentiate facts from commentaries. The following is a headline from CNN : The US-China trade war isn't getting better. This week showed why it could get worse. Including such headlines in our data could affect the reliability of our predictions. Thankfully we are able to find an alternative data source from Wikipedia that offers summaries of actual incidents that occured between countries. Here is an example for the relationship between United States and Singapore retrieved from the summary section of the Singapore-United States Relations wikipedia page:       # # Create empty dataframe# wiki_df = pd. DataFrame(columns = [&#39;Summary&#39;, &#39;URL&#39;, &#39;Country_Pair&#39;])# for c in unique_countries:#  # For each country, identify which country relations are #  # available on wikipedia#  avail_relations = str(c + &quot; relations&quot;)#  # For each available result#  for r in wikipedia. search(avail_relations):#    # Filter out results are not related to the relations of countries#    if &quot;–&quot; in r and &quot;relations&quot; in r:#     try:#      # Retrieve the summary paragraph on the wikipage#      summary = wikipedia. summary(r)#      # Retrieve the url#      url = wikipedia. page(r). url#      wiki_df = wiki_df. append({&#39;Summary&#39;: summary, &#39;URL&#39;: url, \#                   &#39;Country_Pair&#39;: r},ignore_index=True)#     except:#      pass# wiki_df. to_csv(&#39;. /data/Wiki/wiki_relations. csv&#39;, index=False)    Unfortunately, Wikipedia does not cover all 18,336 pairs that we are interested in. We are only able to retrieve the information for 1,463 pairs.       wiki_df = pd. read_csv(&#39;. /data/Wiki/wiki_relations. csv&#39;)print(wiki_df. shape)wiki_df[[&#39;Summary&#39;, &#39;URL&#39;, &#39;Country_Pair&#39;]]. head(10)  (1463, 3)         Summary   URL   Country_Pair         0   Relations between France and Algeria span more. . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–France relations       1   Algeria – United States relations are the inte. . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–United States relations       2   Algeria–Libya relations are longstanding betwe. . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–Libya relations       3   Algeria–Morocco relations have been dominated . . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–Morocco relations       4   Algeria–Turkey relations are foreign relations. . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–Turkey relations       5   Algeria–Russia relations (Russian: Российско–а. . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–Russia relations       6   Algeria–Israel relations refers to the current. . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–Israel relations       7   Algeria–Indonesia relations refers to the bila. . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–Indonesia relations       8   Algeria–China relations (also, Sino-Algerian r. . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–China relations       9   Angola – United States relations are diplomati. . .    https://en. wikipedia. org/wiki/Angola%E2%80%93U. . .    Angola–United States relations     Some of the summaries that we have retrieved from Wikipedia were empty strings. Let's remove them.       wiki_df = wiki_df. fillna(&quot;EMPTY&quot;)wiki_df = wiki_df[wiki_df[&#39;Summary&#39;] != &quot;EMPTY&quot;]    Let's split the pairs in Country_Pairs into their constituent countries and place them in separate columns Country_A and Country_B       def split_country_pair(wiki_country_pairs):  &quot;&quot;&quot;  Split country_pairs found via wikipedia (eg. &quot;Algebria-France relations&quot;)    args:  ------    wiki_country_pairs: (list) country_pairs to be split  Return:  ------    countries_A: (list) countries on the left of &quot;-&quot; separator    countries_B: (list) countries on the right of &quot;-&quot; separator  &quot;&quot;&quot;  country_pairs = [p. replace(&quot; relations&quot;, &quot;&quot;) for p in wiki_country_pairs]  countries_A = [p. split(&quot;–&quot;)[0] for p in country_pairs]  countries_B = [p. split(&quot;–&quot;)[1] for p in country_pairs]  return countries_A , countries_B          wiki_df[&#39;Country_A&#39;], wiki_df[&#39;Country_B&#39;] = split_country_pair(wiki_df[&#39;Country_Pair&#39;]. tolist())    With the country pairs separated we can retrieve their standardized std names. Remember the get_std_names function that we created earlier?       wiki_df = get_std_names(wiki_df, &#39;Country_A&#39;, alt_names_to_countries)wiki_df = get_std_names(wiki_df, &#39;Country_B&#39;, alt_names_to_countries)wiki_df. head(5)           Summary   URL   Country_Pair   Country_A   Country_B   Country_A_Std   Country_B_Std         0   Relations between France and Algeria span more. . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–France relations   Algeria   France   algeria   france       1   Algeria – United States relations are the inte. . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–United States relations   Algeria   United States   algeria   united_states       2   Algeria–Libya relations are longstanding betwe. . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–Libya relations   Algeria   Libya   algeria   libya       3   Algeria–Morocco relations have been dominated . . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–Morocco relations   Algeria   Morocco   algeria   morocco       4   Algeria–Turkey relations are foreign relations. . .    https://en. wikipedia. org/wiki/Algeria%E2%80%93. . .    Algeria–Turkey relations   Algeria   Turkey   algeria   turkey     Generate labeled data for training Creating your own training dataset is one of the most challenging task in the real world. To finetune ALBERT for our use case, we will need to create a training dataset. Due to time limitations, let's label 10% of the data l while the remaining 90% of the data will remain unlabeled ul.       idx = np. array(wiki_df. index. tolist())X_l, X_ul = train_test_split(idx, train_size=0. 10, random_state=42)wiki_l_df = wiki_df. loc[X_l]wiki_ul_df = wiki_df. loc[X_ul]    Splitting wikipedia summary paragraphs into sentences : Currently our training data exists in in the form of a paragraphs. We will can split the summary paragraphs into individual sentences before labeling them.       def split_summaries(summaries, country_As, country_Bs):  &quot;&quot;&quot;  Split wikipedia summaries of each country pair into individual sentences    args:  ------    summaries: (list) wikipedia summaries    country_As: (list) countries in Country_A column    country_Bs: (list) countries in Country_B column  Return:  ------    sent_df: (pd. DataFrame) a new dataframe with each row representing         one sentence of a wikipedia summary  &quot;&quot;&quot;  nlp = English()  nlp. add_pipe(nlp. create_pipe(&#39;sentencizer&#39;))  sents = [sent. text for summary in summaries for sent in nlp(summary). sents]  sents = [s. replace(&quot;\n&quot;, &quot;&quot;) for s in sents]  sents  # We will have to replicate the names of the country as many times  # as the number of sentences  sents_counts = [len(list(nlp(summary). sents)) for summary in summaries]  country_A_freq = list(zip(country_As, sents_counts))  country_B_freq = list(zip(country_Bs, sents_counts))    country_A_rep = list(itertools. chain. from_iterable(itertools. repeat(x[0], x[1]) for x in country_A_freq))  country_B_rep = list(itertools. chain. from_iterable(itertools. repeat(x[0], x[1]) for x in country_B_freq))  # Create dataframe of sentences  d = {&#39;Summary_Sentence&#39;:sents, &#39;Country_A&#39;: country_A_rep, \     &#39;Country_B&#39;: country_B_rep}  sent_df = pd. DataFrame(d)  sent_df[&#39;Country_Pair&#39;] = sent_df[&#39;Country_A&#39;] + &#39;_&#39; + sent_df[&#39;Country_B&#39;]  return sent_df          sent_l_df = split_summaries(wiki_l_df[&#39;Summary&#39;]. tolist(),              wiki_l_df[&#39;Country_A&#39;]. tolist(),              wiki_l_df[&#39;Country_B&#39;]. tolist())sent_ul_df = split_summaries(wiki_ul_df[&#39;Summary&#39;]. tolist(),               wiki_ul_df[&#39;Country_A&#39;]. tolist(),               wiki_ul_df[&#39;Country_B&#39;]. tolist())          sent_l_df. head(10)           Summary_Sentence   Country_A   Country_B   Country_Pair         0   China–Indonesia relations refer to the foreign. . .    China   Indonesia   China_Indonesia       1   The relations between two nations have been on. . .    China   Indonesia   China_Indonesia       2   However, the diplomatic relationship between t. . .    China   Indonesia   China_Indonesia       3   China has an embassy in Jakarta and consulates. . .    China   Indonesia   China_Indonesia       4   Both countries are among the largest nations i. . .    China   Indonesia   China_Indonesia       5   Both nations are the members of APEC and G-20 . . .    China   Indonesia   China_Indonesia       6   According to a 2014 BBC World Service Poll, th. . .    China   Indonesia   China_Indonesia       7   Djibouti – United States relations are bilater. . .    Djibouti   United States   Djibouti_United States       8   Poland–Russia relations (Polish: Stosunki pols. . .    Poland   Russia   Poland_Russia       9   Over centuries, there have been several Polish. . .    Poland   Russia   Poland_Russia     Some data cleaning to do - Removing irrelevant sentences : There are some sentences that do not offer any insights into the relationships between countries. For example: United States – Singapore relations are bilateral relations between the United States and Singapore. It will be good for us to remove them to increase the performance of our model. Unfortunately, these sentences come in various forms. Let's take a look at another example: The Bahamas–United States relations refers to foreign relations between the Bahamas and States There are more examples below if you are interested to look at them. We will use regex to remove them.       regex_1 = &#39;^(The|&#39;&#39;)\s?[a-zA-Z\s]+(–|-|and)[a-zA-Z\s]+ relations refer[s]?&#39;regex_1_df = sent_ul_df[sent_ul_df[&#39;Summary_Sentence&#39;]. str. contains(regex_1, regex=True)]regex_1_df. head(5)[&#39;Summary_Sentence&#39;]. tolist()  /usr/local/lib/python3. 6/dist-packages/pandas/core/strings. py:1843: UserWarning: This pattern has match groups. To actually get the groups, use str. extract.  return func(self, *args, **kwargs)[&#39;French–American relations refers to the diplomatic, social, economic and cultural relations between France and the United States since 1776. &#39;, &#39;Trinidad and Tobago–Venezuela relations refers to the bilateral relations between the Republic of Trinidad and Tobago and the Bolivarian Republic of Venezuela. &#39;, &#39;Bangladesh–Ukraine relations refer to the bilateral relations between Bangladesh and Ukraine. &#39;, &#39;Mauritius–Russia relations refers to the bilateral relations of Russia and Mauritius. &#39;, &#39;Bangladesh–Brunei relations refer to the bilateral relations between Bangladesh and Brunei. &#39;]        regex_2 = &#39;^(The|&#39;&#39;)\s?[a-zA-Z\s]+(–|-|and)[a-zA-Z\s]+ relations (are|is)&#39;regex_2_df = sent_ul_df[sent_ul_df[&#39;Summary_Sentence&#39;]. str. contains(regex_2, regex=True)]regex_2_df. head(5)[&#39;Summary_Sentence&#39;]. tolist()  /usr/local/lib/python3. 6/dist-packages/pandas/core/strings. py:1843: UserWarning: This pattern has match groups. To actually get the groups, use str. extract.  return func(self, *args, **kwargs)[&#39;British–Canadian relations are the relations between Canada and the United Kingdom of Great Britain and Northern Ireland, being bilateral relations between their governments and wider relations between the 2 societies. &#39;, &#39;Bahrain–United Kingdom relations are bilateral relations between Kingdom of Bahrain and the United Kingdom of Great Britain and Northern Ireland. &#39;, &#39;Portugal–Russia relations are foreign relations between Portugal and Russia. &#39;, &#39;Cuba–Mexico relations are the diplomatic and bilateral relations between the Republic of Cuba and the United Mexican States. &#39;, &#39;Kyrgyzstan–Russia relations is the relationship between the two countries, Kyrgyzstan and Russia. &#39;]  remove_invalid_sents will help us to remove all the different combinations that we have spotted so far.       def remove_invalid_sents(df):  # Remove empty sentences  df. replace(&quot;&quot;, np. nan, inplace=True)  df. dropna(subset=[&quot;Summary_Sentence&quot;], inplace=True)  # Remove sentences with . . . relations refers to. . .   regex_1 = &#39;^(The|&#39;&#39;)\s?[a-zA-Z\s]+(–|-|and)[a-zA-Z\s]+ relations refer[s]?&#39;  regex_2 = &#39;^(The|&#39;&#39;)\s?[a-zA-Z\s]+(–|-|and)[a-zA-Z\s]+ relations (are|is)&#39;  df = df[~df[&#39;Summary_Sentence&#39;]. str. contains(regex_1, regex=True)]  df = df[~df[&#39;Summary_Sentence&#39;]. str. contains(regex_2, regex=True)]    return df          sent_l_df = remove_invalid_sents(sent_l_df)sent_ul_df = remove_invalid_sents(sent_ul_df)  /usr/local/lib/python3. 6/dist-packages/pandas/core/strings. py:1843: UserWarning: This pattern has match groups. To actually get the groups, use str. extract.  return func(self, *args, **kwargs)  For now, we will exclude the summaries between  History of Japan  and  Korea  and instead analyze the relationship between  Japan  and  Korea        hoj = sent_l_df[sent_l_df[&#39;Country_A&#39;] == &#39;History of Japan&#39;]hoj. head(5)           Summary_Sentence   Country_A   Country_B   Country_Pair         14   For over 15 centuries, the relationship betwee. . .    History of Japan   Korea   History of Japan_Korea       15   During the ancient era, exchanges of cultures . . .    History of Japan   Korea   History of Japan_Korea       16   Buddhism, Chinese-influenced cuisine, Han char. . .    History of Japan   Korea   History of Japan_Korea       17   Since 1945, relations involve three states: No. . .    History of Japan   Korea   History of Japan_Korea       18   Japan cut off Korea from Qing Chinese suzerain. . .    History of Japan   Korea   History of Japan_Korea     Let's save the data we have created into csv files.       # sent_l_df. to_csv(&#39;. /data/Wiki/sent_l. csv&#39;, index=False)sent_ul_df. to_csv(&#39;. /data/Wiki/sent_ul. csv&#39;, index=False)    I have manually labeled close to 700 sentences, giving them a label of 0, 1 and 2 for negative, positive and neutral sentiments respectively. Let's read the file to obtain the labels ld.       sent_ld_all_df = pd. read_csv(&#39;. /data/Wiki/sent_ld. csv&#39;)sent_ld_all_df. head(10)           Summary_Sentence   Label   Country_A   Country_B   Country_Pair         0   The relations between two nations have been on. . .    1   China   Indonesia   China_Indonesia       1   However, the diplomatic relationship between t. . .    0   China   Indonesia   China_Indonesia       2   China has an embassy in Jakarta and consulates. . .    1   China   Indonesia   China_Indonesia       3   Both countries are among the largest nations i. . .    2   China   Indonesia   China_Indonesia       4   Both nations are the members of APEC and G-20 . . .    1   China   Indonesia   China_Indonesia       5   \nAccording to a 2014 BBC World Service Poll, . . .    1   China   Indonesia   China_Indonesia       6   Poland–Russia relations (Polish: Stosunki pols. . .    0   Poland   Russia   Poland_Russia       7   Over centuries, there have been several Polish. . .    0   Poland   Russia   Poland_Russia       8   Polish–Russian relations entered a new phase f. . .    2   Poland   Russia   Poland_Russia       9   Since then Polish–Russian relations have at ti. . .    2   Poland   Russia   Poland_Russia     Let's step ahead a little. While working on this blog post, I did realise that the inclusion of sentences with neutral sentiments affects the performance of the model negatively. Below are some examples of these sentences. For now, let's exclude them and only train our model with sentences of positive and negative sentiments.       sent_ld_neutral_df = sent_ld_all_df[sent_ld_all_df[&#39;Label&#39;] == 2]sent_ld_neutral_df[&#39;Summary_Sentence&#39;]. tolist()[0:5]  [&#39;Both countries are among the largest nations in Asia in terms of both area and population, China is the most populous nation on earth, while Indonesia has the 4th largest population in the world. &#39;, &#39;Polish–Russian relations entered a new phase following the fall of communism, 1989–1993. &#39;, &#39;Since then Polish–Russian relations have at times seen both improvement and deterioration. &#39;, &#39;The relationship was generally warm under the Russian President Boris Yeltsin (1991–99) until the NATO bombing of the Federal Republic of Yugoslavia in the spring of 1999, and has since deteriorated significantly. &#39;, &#39;Relations between the neighboring countries of Iran and the United Arab Emirates (UAE) are deeply historic, dating back centuries prior to the establishment of the modern-day United Arab Emirates; however today it has been described as up-and-down. &#39;]        sent_ld_df = sent_ld_all_df[sent_ld_all_df[&#39;Label&#39;] != 2]. reset_index(drop=True)    Generating training and development datasets To tune our hyperparameters and prevent over/underfitting, we will need to generate a development (validation) dataset. While splitting the data, there is a need for us to split by country pairs to prevent any leakage in information about the countries' relations leaking from the training to development set.       country_pairs = sent_ld_df[&#39;Country_Pair&#39;]. unique()country_pairs_to_num = {v:i for i,v in enumerate(country_pairs)}sent_ld_df[&#39;Group&#39;] = sent_ld_df[&#39;Country_Pair&#39;]. map(country_pairs_to_num)idx = np. array(sent_ld_df. index. tolist())groupings = np. array(sent_ld_df[&#39;Group&#39;]. tolist())gss = GroupShuffleSplit(n_splits=1, test_size=0. 2, random_state=42)for idx_1, idx_2 in gss. split(idx, groups=groupings): train_idx = idx_1 dev_idx = idx_2    Let's save the training and development datasets into tsv files.       train_data_df = sent_ld_df[[&#39;Summary_Sentence&#39;, &#39;Label&#39;]]. loc[train_idx]dev_data_df = sent_ld_df[[&#39;Summary_Sentence&#39;, &#39;Label&#39;]]. loc[dev_idx]train_data_df. to_csv(&#39;. /data/Wiki/train. tsv&#39;, float_format=&#39;%. 0f&#39;, \           encoding=&#39;utf-8&#39;, header=False, index=False, sep=&#39;\t&#39;)dev_data_df. to_csv(&#39;. /data/Wiki/dev. tsv&#39;, float_format=&#39;%. 0f&#39;, \          encoding=&#39;utf-8&#39;, header=False, index=False, sep=&#39;\t&#39;)    Finetuning ALBERT. . . Finally! It is true when people say that in the field of Data Science, you spend most of the time analyzing and cleaning the data. We probably spent half of the blog post preparing the data for this section. Now it is time for the exciting bit! We will create an Albert class that we will use to generate our sentiments. There are a couple of helper functions that Albert will be using. Most of the helper functions are adapted from the Huggingface github page. If there any tips on how I can do this better, do drop me an email! The helper functions _create_examples, _convert_examples_to_features, _read_tsv, create_dataset are primarily used to convert our training and development datasets into a format that can be ingested by the Huggingface's model. Do check out this Huggingface github page if you are interested in understanding the InputExample and InputFeatures data types. _plot_cost_history will be used to plot the training and development loss/accuracies to see how how our model is performing.       def _create_examples(lines, set_type, col_num_text, col_num_label):  &quot;&quot;&quot;  Helper function for Albert&#39;s preprocess_str method.   Create examples for the training and dev sets  args:  ------    lines: (list) contains the guid, text to be labeled, and the label     set_type: (str) denotes whether the lines are training or dev data         eg. &quot;train&quot;, &quot;dev&quot;  Return:  ------    examples: (list) contains training/dev examples (InputExample object)            for sequence classification  &quot;&quot;&quot;  examples = []  for (i, line) in enumerate(lines):    if i == 0:     pass    else:     guid = &quot;%s-%s&quot; % (set_type, i)     text_a = line[col_num_text] # To check if this is index 0     label = line[col_num_label]     examples. append(utils. InputExample(guid=guid, text_a=text_a, \                       text_b=None, label=label))      return examples          def _convert_examples_to_features(examples, tokenizer, max_length, task):  &quot;&quot;&quot;  Helper function for Albert&#39;s preprocess_str method.   Convert data from examples to features  args:  ------    examples: (list) contains training/dev examples ``InputExample``         for sequence classification  Return:  ------    features: (list) contains task-specific ``InputFeatures``         which can be fed to the Albert model.   &quot;&quot;&quot;  label_list = [&quot;0&quot;,&quot;1&quot;]  features = glue_convert_examples_to_features(examples=examples, \                        label_list=label_list, \                        tokenizer=tokenizer, \                        max_length=max_length, task=task)    return features          def _read_tsv(file_path):  &quot;&quot;&quot;  Helper function for Albert&#39;s preprocess_str method.   Reads a tab separated value file.   args:  ------    file_path: (str) path leading to the tab separated tsv file  Return:  ------    list with each element representing a row in the tsv file  &quot;&quot;&quot;  with open(file_path, &quot;r&quot;, encoding=&quot;utf-8-sig&quot;) as f:    return list(csv. reader(f, delimiter=&quot;\t&quot;))          def _create_dataset(features, buffer_size, batch_size):  &quot;&quot;&quot;  Helper function for Albert&#39;s preprocess_str method.   Convert features into ``tf. data. Dataset``.   args:  ------    features: (list) contains task-specific ``InputFeatures`` which can be         fed to the model.     buffer_size: (int) representing the number of elements from this dataset          which the new dataset will sample from    batch_size: (int) size of each batch generated from the dataset  Return:  ------    A ``tf. data. Dataset`` containining task specific features  &quot;&quot;&quot;  data_size = len(features)  def gen():    for ex in features:      yield (        {          &quot;input_ids&quot;: ex. input_ids,          &quot;attention_mask&quot;: ex. attention_mask,          &quot;token_type_ids&quot;: ex. token_type_ids,        },        ex. label,      )  train_dataset = tf. data. Dataset. from_generator(    gen,    ({&quot;input_ids&quot;: tf. int32, &quot;attention_mask&quot;: tf. int32, &quot;token_type_ids&quot;: tf. int32}, tf. int64),    (      {        &quot;input_ids&quot;: tf. TensorShape([None]),        &quot;attention_mask&quot;: tf. TensorShape([None]),        &quot;token_type_ids&quot;: tf. TensorShape([None]),      },      tf. TensorShape([]),    ),  )  return train_dataset. shuffle(buffer_size). batch(batch_size). repeat(-1)          def _plot_cost_history(history):  &quot;&quot;&quot;  Helper function for Albert&#39;s fintune method.   Plots a line graph illustrating the loss for each epoch.   args:  ------    history: (tf history object) contains the loss parameters to be charted    epochs: (int) number of epochs. Used as the x-axis of the graph  Return:  ------    A ``tf. data. Dataset`` containining task specific features  &quot;&quot;&quot;  # Get number of epochs  n_epochs = len(history. history[&#39;accuracy&#39;])  # Top plot  plt. subplot(2,1,1)  plt. title(&#39;Accuracy/Loss&#39;)  plt. plot(history. history[&#39;accuracy&#39;])  plt. plot(history. history[&#39;val_accuracy&#39;])  plt. ylabel(&#39;Accuracy&#39;)  plt. xticks(np. arange(n_epochs))  # Bottom plot  plt. subplot(2,1,2)  plt. plot(history. history[&#39;loss&#39;])  plt. plot(history. history[&#39;val_loss&#39;])  plt. ylabel(&#39;Loss&#39;)  plt. tight_layout()  # Get number of epochs  n_epochs = len(history. history[&#39;accuracy&#39;])  plt. xticks(np. arange(n_epochs))  plt. legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;upper left&#39;)  plt. savefig(&#39;train_val. png&#39;)          class Albert:  def __init__(self, **kwargs):    for key, value in kwargs. items():     setattr(self, key, value)    albert4Seq = TFAlbertForSequenceClassification. from_pretrained(self. albert_config, output_attentions=True)    optimizer = tf. keras. optimizers. Adam(learning_rate=self. lr)    loss = tf. keras. losses. SparseCategoricalCrossentropy(from_logits=True)    metric = tf. keras. metrics. SparseCategoricalAccuracy(&#39;accuracy&#39;)    albert4Seq. compile(optimizer=optimizer, loss=loss, metrics=[metric])    if self. load_weights_path:     albert4Seq. load_weights(self. load_weights_path)     albert4Seq. optimizer. lr. assign(self. lr)        self. embed = albert4Seq    self. tokenizer = AlbertTokenizer. from_pretrained(self. albert_config)    print(&#39;model initiated!&#39;)   def preprocess_str(self, train_data_path, dev_data_path):    &quot;&quot;&quot;    Reads and converts training and dev data from a . tsv file into a    tensorflow dataset    args:    ------      train_data_path: (str) path leading to the training . tsv file      dev_data_path: (int) path leading to the dev . tsv file    Return:    ------      train_dataset: (tf. data. Dataset) containining task specific training             features      dev_dataset: (tf. data. Dataset) containining task specific dev features      train_size: (int) number of training data      dev_size: (int) number of dev data    &quot;&quot;&quot;    train_raw = _read_tsv(train_data_path)    dev_raw = _read_tsv(dev_data_path)    train_size = len(train_raw)    dev_size = len(dev_raw)    train_examples = _create_examples(train_raw,                     &quot;train&quot;,                     self. col_num_text_a,                     self. col_num_label)        dev_examples = _create_examples(dev_raw,                    &quot;dev&quot;,                    self. col_num_text_a,                    self. col_num_label)        train_features = _convert_examples_to_features(train_examples,                            self. tokenizer,                            self. max_seq_length,                            self. task)        dev_features = _convert_examples_to_features(dev_examples,                           self. tokenizer,                           self. max_seq_length,                           self. task)        train_dataset = _create_dataset(train_features,                    self. buffer_size,                    self. batch_size)        dev_dataset = _create_dataset(dev_features,                   self. buffer_size,                   self. batch_size)        return train_dataset, dev_dataset, train_size, dev_size  def finetune(self):    &quot;&quot;&quot;    Finetunes the Albert model    &quot;&quot;&quot;    train_dataset, dev_dataset, self. train_size, \    self. dev_size = self. preprocess_str(train_data_path=self. train_data_path,                      dev_data_path=self. dev_data_path)        print(&#39;dataset loaded!&#39;)    # Callbacks    reduce_lr = ReduceLROnPlateau(monitor=&#39;val_loss&#39;,                   factor=self. reduce_lr_factor,                   patience=self. lr_patience,                   min_lr=self. min_lr)    checkpoint = ModelCheckpoint(self. checkpoint_save_path,                   monitor=&#39;val_loss&#39;,                   verbose=1,                   save_best_only=True,                   mode=&#39;min&#39;)    early_stop = EarlyStopping(monitor=&#39;val_loss&#39;,                  patience=self. early_stop_patience,                  restore_best_weights=True)    # Model    epochs = self. num_epochs    train_steps = self. train_size // self. batch_size    valid_steps = self. dev_size // self. batch_size    albert_history = self. embed. fit(train_dataset,                     epochs=epochs,                     steps_per_epoch=train_steps,                    validation_data=dev_dataset,                    validation_steps=valid_steps,                    callbacks=[reduce_lr,                         checkpoint,                         early_stop])    # Plot loss    _plot_cost_history(albert_history)  def export(self, file_path=&#39;&#39;):    &quot;&quot;&quot;    Export finetuned model    args:    ------      file_path: (str) path leading to the directory to save the            finetuned model    &quot;&quot;&quot;    self. embed. save_weights(file_path)  def restore(self, file_path=&#39;&#39;):    &quot;&quot;&quot;    Restore Albert model with finetuned model    args:    ------      file_path: (str) path leading to the directory with finetuned model    &quot;&quot;&quot;    try:      self. embed. load_weights(file_path)    except Exception as e:      print(str(e))    Finetuning on SQUAD SST-2 Task : Huggingface offers pretrained models that need to be finetuned with your own use case. At this point, the Huggingface ALBERT model is not capable of differentiating between positive and negative sentiments. For ALBERT to learn how to differentiate the two types of sentiment, we will need to first finetune Albert using SQUAD's SST-2 dataset. The SST-2 dataset contains sentences from movie reviews and human annotations of their sentiments. Here are two examples adapted from the dataset: Negative sentiment -  contains no wit , only labored gags  Positive sentiment -  are more deeply thought through than in most right-thinking films  Once, we have trained a model capable of differentiating negative and sentiments by analyzing movie reviews, we will implement transfer learning to further finetune the model with our own Wikipedia summary dataset. To download the SST-2 dataset, we will use Huggingface's download_and_extract. You can use the same function to download other SQUAD related datasets that you would like to finetune on.       TASKS = [&quot;CoLA&quot;, &quot;SST&quot;, &quot;MRPC&quot;, &quot;QQP&quot;, &quot;STS&quot;, &quot;MNLI&quot;, &quot;SNLI&quot;, &quot;QNLI&quot;, &quot;RTE&quot;, &quot;WNLI&quot;, &quot;diagnostic&quot;]TASK2PATH = {&quot;CoLA&quot;:&#39;https://firebasestorage. googleapis. com/v0/b/mtl-sentence-representations. appspot. com/o/data%2FCoLA. zip?alt=media&amp;token=46d5e637-3411-4188-bc44-5809b5bfb5f4&#39;,       &quot;SST&quot;:&#39;https://firebasestorage. googleapis. com/v0/b/mtl-sentence-representations. appspot. com/o/data%2FSST-2. zip?alt=media&amp;token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8&#39;,       &quot;MRPC&quot;:&#39;https://firebasestorage. googleapis. com/v0/b/mtl-sentence-representations. appspot. com/o/data%2Fmrpc_dev_ids. tsv?alt=media&amp;token=ec5c0836-31d5-48f4-b431-7480817f1adc&#39;,       &quot;QQP&quot;:&#39;https://firebasestorage. googleapis. com/v0/b/mtl-sentence-representations. appspot. com/o/data%2FQQP. zip?alt=media&amp;token=700c6acf-160d-4d89-81d1-de4191d02cb5&#39;,       &quot;STS&quot;:&#39;https://firebasestorage. googleapis. com/v0/b/mtl-sentence-representations. appspot. com/o/data%2FSTS-B. zip?alt=media&amp;token=bddb94a7-8706-4e0d-a694-1109e12273b5&#39;,       &quot;MNLI&quot;:&#39;https://firebasestorage. googleapis. com/v0/b/mtl-sentence-representations. appspot. com/o/data%2FMNLI. zip?alt=media&amp;token=50329ea1-e339-40e2-809c-10c40afff3ce&#39;,       &quot;SNLI&quot;:&#39;https://firebasestorage. googleapis. com/v0/b/mtl-sentence-representations. appspot. com/o/data%2FSNLI. zip?alt=media&amp;token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df&#39;,       &quot;QNLI&quot;: &#39;https://firebasestorage. googleapis. com/v0/b/mtl-sentence-representations. appspot. com/o/data%2FQNLIv2. zip?alt=media&amp;token=6fdcf570-0fc5-4631-8456-9505272d1601&#39;,       &quot;RTE&quot;:&#39;https://firebasestorage. googleapis. com/v0/b/mtl-sentence-representations. appspot. com/o/data%2FRTE. zip?alt=media&amp;token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb&#39;,       &quot;WNLI&quot;:&#39;https://firebasestorage. googleapis. com/v0/b/mtl-sentence-representations. appspot. com/o/data%2FWNLI. zip?alt=media&amp;token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf&#39;,       &quot;diagnostic&quot;:&#39;https://storage. googleapis. com/mtl-sentence-representations. appspot. com/tsvsWithoutLabels%2FAX. tsv?GoogleAccessId=firebase-adminsdk-0khhl@mtl-sentence-representations. iam. gserviceaccount. com&amp;Expires=2498860800&amp;Signature=DuQ2CSPt2Yfre0C%2BiISrVYrIFaZH1Lc7hBVZDD4ZyR7fZYOMNOUGpi8QxBmTNOrNPjR3z1cggo7WXFfrgECP6FBJSsURv8Ybrue8Ypt%2FTPxbuJ0Xc2FhDi%2BarnecCBFO77RSbfuz%2Bs95hRrYhTnByqu3U%2FYZPaj3tZt5QdfpH2IUROY8LiBXoXS46LE%2FgOQc%2FKN%2BA9SoscRDYsnxHfG0IjXGwHN%2Bf88q6hOmAxeNPx6moDulUF6XMUAaXCSFU%2BnRO2RDL9CapWxj%2BDl7syNyHhB7987hZ80B%2FwFkQ3MEs8auvt5XW1%2Bd4aCU7ytgM69r8JDCwibfhZxpaa4gd50QXQ%3D%3D&#39;}def download_and_extract(task, file_path):  &quot;&quot;&quot;  Used to download SQUAD dataset for training.   Credits to Huggingface  args:  ------    task: (str) SQUAD task of interest    file_path: (str) path to save the dataset in  &quot;&quot;&quot;  print(&quot;Downloading and extracting %s. . . &quot; % task)  data_file = &quot;%s. zip&quot; % task  urllib. request. urlretrieve(TASK2PATH[task], data_file)  with zipfile. ZipFile(data_file) as zip_ref:    zip_ref. extractall(file_path)  os. remove(data_file)  print(&quot;\tCompleted!&quot;) download_and_extract(&quot;SST&quot;, &#39;. /data/&#39;)  Downloading and extracting SST. . . 	Completed!  The data files and hyperparameters needed to finetune Albert on the SQUAD SST-2 task and our wikipedia summaries are different. To easily facilitate the different training requirements, we will wrap the data paths and parameters in a config dictionary. Which will be used to instantiate Albert before finetuning begins. The config to finetune on SQUAD:       config_squad = {  &quot;albert_config&quot;: &quot;albert-base-v2&quot;,  &quot;task&quot;: &quot;sst-2&quot;,  &quot;max_seq_length&quot;: 128,  &quot;buffer_size&quot;: 128,  &quot;batch_size&quot;: 36,  &quot;test_size&quot;: 0. 3,  &quot;random_state&quot;: 42,  &quot;load_weights_path&quot;: &quot;&quot;,  &quot;lr&quot;: 5e-6,  &quot;reduce_lr_factor&quot;: 0. 1,  &quot;lr_patience&quot;: 1,  &quot;early_stop_patience&quot;: 3,  &quot;num_epochs&quot;: 10,  &quot;min_lr&quot;: 5e-9,  &quot;train_data_path&quot;: &quot;. /data/SST-2/train. tsv&quot;,  &quot;dev_data_path&quot;: &quot;. /data/SST-2/dev. tsv&quot;,  &quot;col_num_text_a&quot;: 0,  &quot;col_num_label&quot;: 1,  &quot;checkpoint_save_path&quot;: &quot;. /model/squad-weights-improvement-{epoch:02d}. ckpt&quot;}    The finetuning logs can be seen below. The fourth epoch seems to be our best run with the lowest val_loss (0. 4706).       # albert_squad = Albert(**config_squad)# albert_squad. finetune()  model initiated!dataset loaded!Train for 1870 steps, validate for 24 stepsEpoch 1/101869/1870 [============================&gt;. ] - ETA: 0s - loss: 0. 5781 - accuracy: 0. 6841Epoch 00001: val_loss improved from inf to 0. 55461, saving model to . /model/squad-weights-improvement-01. ckpt1870/1870 [==============================] - 1013s 542ms/step - loss: 0. 5780 - accuracy: 0. 6841 - val_loss: 0. 5546 - val_accuracy: 0. 7222Epoch 2/101869/1870 [============================&gt;. ] - ETA: 0s - loss: 0. 4043 - accuracy: 0. 8158Epoch 00002: val_loss improved from 0. 55461 to 0. 52056, saving model to . /model/squad-weights-improvement-02. ckpt1870/1870 [==============================] - 989s 529ms/step - loss: 0. 4043 - accuracy: 0. 8158 - val_loss: 0. 5206 - val_accuracy: 0. 7708Epoch 3/101869/1870 [============================&gt;. ] - ETA: 0s - loss: 0. 3226 - accuracy: 0. 8628Epoch 00003: val_loss did not improve from 0. 520561870/1870 [==============================] - 990s 530ms/step - loss: 0. 3226 - accuracy: 0. 8628 - val_loss: 0. 5379 - val_accuracy: 0. 7801Epoch 4/101869/1870 [============================&gt;. ] - ETA: 0s - loss: 0. 2661 - accuracy: 0. 8911Epoch 00004: val_loss improved from 0. 52056 to 0. 47058, saving model to . /model/squad-weights-improvement-04. ckpt1870/1870 [==============================] - 990s 529ms/step - loss: 0. 2662 - accuracy: 0. 8911 - val_loss: 0. 4706 - val_accuracy: 0. 8032Epoch 5/101869/1870 [============================&gt;. ] - ETA: 0s - loss: 0. 2596 - accuracy: 0. 8944Epoch 00005: val_loss did not improve from 0. 470581870/1870 [==============================] - 989s 529ms/step - loss: 0. 2596 - accuracy: 0. 8944 - val_loss: 0. 4746 - val_accuracy: 0. 8021Epoch 6/101869/1870 [============================&gt;. ] - ETA: 0s - loss: 0. 2532 - accuracy: 0. 8974Epoch 00006: val_loss did not improve from 0. 470581870/1870 [==============================] - 989s 529ms/step - loss: 0. 2533 - accuracy: 0. 8973 - val_loss: 0. 4724 - val_accuracy: 0. 8067Epoch 7/101869/1870 [============================&gt;. ] - ETA: 0s - loss: 0. 2526 - accuracy: 0. 8976Epoch 00007: val_loss did not improve from 0. 470581870/1870 [==============================] - 989s 529ms/step - loss: 0. 2526 - accuracy: 0. 8976 - val_loss: 0. 4764 - val_accuracy: 0. 8044  Finetune using our labeled dataset : Ok we have finetuned Albert to pick out the positive and negative sentiments movie reviews. Finally, we will finetune Albert for our use case using the following configurations. The eighth epoch gave the best run with the lowest loss (0. 4546).       config_summaries = {  &quot;albert_config&quot;: &quot;albert-base-v2&quot;,  &quot;task&quot;: &quot;sst-2&quot;,  &quot;max_seq_length&quot;: 128,  &quot;buffer_size&quot;: 128,  &quot;batch_size&quot;: 36,  &quot;test_size&quot;: 0. 3,  &quot;random_state&quot;: 42,  &quot;load_weights_path&quot;: &quot;. /model/squad-weights-improvement-04. ckpt&quot;,  &quot;lr&quot;: 5e-5,  &quot;reduce_lr_factor&quot;: 0. 1,  &quot;lr_patience&quot;: 2,  &quot;early_stop_patience&quot;: 5,  &quot;num_epochs&quot;: 20,  &quot;min_lr&quot;: 5e-9,  &quot;train_data_path&quot;: &quot;. /data/Wiki/train. tsv&quot;,  &quot;dev_data_path&quot;: &quot;. /data/Wiki/dev. tsv&quot;,  &quot;col_num_text_a&quot;: 0,  &quot;col_num_label&quot;: 1,  &quot;checkpoint_save_path&quot;: &quot;. /model/summary-weights-improvement-{epoch:02d}. ckpt&quot;}          # albert_summaries = Albert(**config_summaries)# albert_summaries. finetune()  model initiated!dataset loaded!Train for 10 steps, validate for 3 stepsEpoch 1/20 9/10 [==========================&gt;. . . ] - ETA: 2s - loss: 0. 6752 - accuracy: 0. 7068Epoch 00001: val_loss improved from inf to 0. 70040, saving model to . /model/summary-weights-improvement-01. ckpt10/10 [==============================] - 30s 3s/step - loss: 0. 6479 - accuracy: 0. 7306 - val_loss: 0. 7004 - val_accuracy: 0. 6204Epoch 2/20 9/10 [==========================&gt;. . . ] - ETA: 0s - loss: 0. 5817 - accuracy: 0. 7081Epoch 00002: val_loss improved from 0. 70040 to 0. 57846, saving model to . /model/summary-weights-improvement-02. ckpt10/10 [==============================] - 7s 661ms/step - loss: 0. 5813 - accuracy: 0. 7095 - val_loss: 0. 5785 - val_accuracy: 0. 7778Epoch 3/20 9/10 [==========================&gt;. . . ] - ETA: 0s - loss: 0. 3281 - accuracy: 0. 8975Epoch 00003: val_loss did not improve from 0. 5784610/10 [==============================] - 6s 600ms/step - loss: 0. 3277 - accuracy: 0. 8883 - val_loss: 0. 9524 - val_accuracy: 0. 6574Epoch 4/20 9/10 [==========================&gt;. . . ] - ETA: 0s - loss: 0. 2239 - accuracy: 0. 9317Epoch 00004: val_loss did not improve from 0. 5784610/10 [==============================] - 6s 607ms/step - loss: 0. 2173 - accuracy: 0. 9330 - val_loss: 0. 5976 - val_accuracy: 0. 7593Epoch 5/20 9/10 [==========================&gt;. . . ] - ETA: 0s - loss: 0. 0883 - accuracy: 0. 9720Epoch 00005: val_loss improved from 0. 57846 to 0. 54417, saving model to . /model/summary-weights-improvement-05. ckpt10/10 [==============================] - 7s 652ms/step - loss: 0. 0857 - accuracy: 0. 9749 - val_loss: 0. 5442 - val_accuracy: 0. 8056Epoch 6/20 9/10 [==========================&gt;. . . ] - ETA: 0s - loss: 0. 0800 - accuracy: 0. 9752Epoch 00006: val_loss improved from 0. 54417 to 0. 49869, saving model to . /model/summary-weights-improvement-06. ckpt10/10 [==============================] - 7s 693ms/step - loss: 0. 0787 - accuracy: 0. 9749 - val_loss: 0. 4987 - val_accuracy: 0. 7963Epoch 7/20 9/10 [==========================&gt;. . . ] - ETA: 0s - loss: 0. 0740 - accuracy: 0. 9814Epoch 00007: val_loss did not improve from 0. 4986910/10 [==============================] - 6s 604ms/step - loss: 0. 0701 - accuracy: 0. 9832 - val_loss: 0. 5500 - val_accuracy: 0. 7593Epoch 8/20 9/10 [==========================&gt;. . . ] - ETA: 0s - loss: 0. 0639 - accuracy: 0. 9783Epoch 00008: val_loss improved from 0. 49869 to 0. 45457, saving model to . /model/summary-weights-improvement-08. ckpt10/10 [==============================] - 7s 678ms/step - loss: 0. 0592 - accuracy: 0. 9804 - val_loss: 0. 4546 - val_accuracy: 0. 8148Epoch 9/20 9/10 [==========================&gt;. . . ] - ETA: 0s - loss: 0. 0410 - accuracy: 0. 9876Epoch 00009: val_loss did not improve from 0. 4545710/10 [==============================] - 6s 599ms/step - loss: 0. 0380 - accuracy: 0. 9888 - val_loss: 0. 5704 - val_accuracy: 0. 7778Epoch 10/20 9/10 [==========================&gt;. . . ] - ETA: 0s - loss: 0. 0323 - accuracy: 0. 9876Epoch 00010: val_loss did not improve from 0. 4545710/10 [==============================] - 6s 599ms/step - loss: 0. 0301 - accuracy: 0. 9888 - val_loss: 0. 5205 - val_accuracy: 0. 8056Epoch 11/20 9/10 [==========================&gt;. . . ] - ETA: 0s - loss: 0. 0259 - accuracy: 0. 9907Epoch 00011: val_loss did not improve from 0. 4545710/10 [==============================] - 6s 588ms/step - loss: 0. 0248 - accuracy: 0. 9916 - val_loss: 0. 5801 - val_accuracy: 0. 7778Epoch 12/20 9/10 [==========================&gt;. . . ] - ETA: 0s - loss: 0. 0183 - accuracy: 0. 9969Epoch 00012: val_loss did not improve from 0. 4545710/10 [==============================] - 6s 597ms/step - loss: 0. 0235 - accuracy: 0. 9917 - val_loss: 0. 5456 - val_accuracy: 0. 7870Epoch 13/20 9/10 [==========================&gt;. . . ] - ETA: 0s - loss: 0. 0170 - accuracy: 0. 9969Epoch 00013: val_loss did not improve from 0. 4545710/10 [==============================] - 6s 595ms/step - loss: 0. 0181 - accuracy: 0. 9972 - val_loss: 0. 5932 - val_accuracy: 0. 7778  Predict Sentiment With the model finetuned with our labeled dataset, we can now use it to come up with sentiment predictions. The sentences need to be encoded before feeding it into the model to obtain the logits logit = albert_summaries. embed(encoded_s)[0]. We will then place the logits through a softmax function _get_proba to obtain the probabilities that the sentiments are negative or positive.       def _get_proba(logit):  &quot;&quot;&quot;  Helper function for get_predictions_probas  Computes softmax values based on logit  args:  ------    logit: (float) logit output of finetuned ALBERT model  Return:  ------    softmax values representing the sentiment probabilities  &quot;&quot;&quot;  return np. exp(logit) / np. sum(np. exp(logit), axis=1)          def get_predictions_probas(summaries, config, weights_path):  &quot;&quot;&quot;  Returns the sentiment probabilities (positive or negative sentiments)  for the wiki summary sentences  args:  ------    summaries: (list) wiki summary sentences that require sentiment predictions    config: (dict) chosen configuration for the ALBERT model    weights_path: (str) path leading to the weights of the finetuned           ALBERT model  Return:  ------    sentiment_preds: (list) sentiment predictions for the wiki summary sentences  &quot;&quot;&quot;  albert_summaries = Albert(**config)  albert_summaries. restore(weights_path)  sentiment_probas = []  for i, s in enumerate(summaries):   encoded_s = albert_summaries. tokenizer. encode_plus(s,                            text_pair=None,                            max_length=albert_summaries. max_seq_length,                            pad_to_max_length=True,                            add_special_tokens=True,                            return_tensors=&quot;tf&quot;)     logit = albert_summaries. embed(encoded_s)[0]   pred = _get_proba(logit)   sentiment_probas. append(pred[:,1][0])  return sentiment_probas          %%capturesent_ld_df = pd. read_csv(&#39;. /data/Wiki/sent_ld. csv&#39;)sent_ul_df = pd. read_csv(&#39;. /data/Wiki/sent_ul. csv&#39;)sent_all_df = pd. concat([sent_ld_df, sent_ul_df], axis=0)summaries = sent_all_df[&#39;Summary_Sentence&#39;]. tolist()sentiment_probas = get_predictions_probas(summaries, config_summaries,                     &#39;. /model/summary-weights-improvement-08. ckpt&#39;)sent_all_df[&#39;Predictions_Probas&#39;] = sentiment_probas    And here we have the sentiment probabilities for the sentences! Sentences with probabilities that are closer to 1 are considered to have positive sentiments.       sent_all_df[[&#39;Country_Pair&#39;, &#39;Label&#39;, &#39;Summary_Sentence&#39;, &#39;Predictions_Probas&#39;]]. head(5)           Country_Pair   Label   Summary_Sentence   Predictions_Probas         0   China_Indonesia   1. 0   The relations between two nations have been on. . .    0. 279244       1   China_Indonesia   0. 0   However, the diplomatic relationship between t. . .    0. 052571       2   China_Indonesia   1. 0   China has an embassy in Jakarta and consulates. . .    0. 990907       3   China_Indonesia   2. 0   Both countries are among the largest nations i. . .    0. 990101       4   China_Indonesia   1. 0   Both nations are the members of APEC and G-20 . . .    0. 985640     How did we do? On to the results! To be honest, I do not know how the results will turn out at this point given that the dataset that we were using was self-generated and manually labeled. On a positive note, it appears that the model was able to predict some of the sentiments correctly. Correctly classified relationships :       # Sentencesprint(sent_all_df[sent_all_df[&#39;Country_Pair&#39;] == &#39;China_Singapore&#39;][&#39;Summary_Sentence&#39;]. tolist())# Sentiment probabilitiessent_all_df[sent_all_df[&#39;Country_Pair&#39;] == &#39;China_Singapore&#39;][&#39;Predictions_Probas&#39;]. tolist()  [&#34;People&#39;s Republic of China – Singapore relations officially started on 3 October 1990. &#34;, &#39;Diplomatic missions were established in the early 1990s based on trade and the warming of ties from other ASEAN countries towards mainland China. &#39;, &#39;Singapore and China have maintained a long-standing and greatly prioritised close relationship, and partly because of the latter\&#39;s growing influence and essentiality in the Asia-Pacific region, specifying that &#34;its common interest with China is far greater than any differences&#34;. &#39;, &#34;Furthermore, Singapore has positioned itself as a strong supporter for China&#39;s constructive engagement and peaceful development in the region. &#34;, &#34;It has engaged co-operation with other ASEAN members and China to strengthen regional security and fight terrorism, while participating in the organisation&#39;s first maritime exercise with the latter. &#34;, &#34;While relationship between the two countries stand strong, differences were experienced during numerous high-profile events, including Singapore&#39;s stance against China regarding the South China Sea dispute, Singapore&#39;s support for the United States&#39; military presence and alliance system in Asia and the seizing of SAF vehicles by Hong Kong authorities in November 2016. Despite the disputes, Singapore and Beijing have consistently affirm their unwavering close relationship and bilateral ties, deepening their co-operation in numerous areas, including defence, economy, culture and education, as well as One Belt One Road Initiative. &#34;, &#34;Singapore has also vowed to fully support and promote China&#39;s position in ASEAN, while managing the differences between the Chinese state and the organisation. &#34;][0. 9891862273216248, 0. 98930823802948, 0. 9773917198181152, 0. 990622878074646, 0. 9888237714767456, 0. 9899033308029175, 0. 9854150414466858]        # Sentencesprint(sent_all_df[sent_all_df[&#39;Country_Pair&#39;] == &#39;Russia_United States&#39;][&#39;Summary_Sentence&#39;]. tolist()[0:4])# Sentiment probabilitiessent_all_df[sent_all_df[&#39;Country_Pair&#39;] == &#39;Russia_United States&#39;][&#39;Predictions_Probas&#39;]. tolist()[0:4]  [&#39;The United States and Russia maintain diplomatic and trade relations. &#39;, &#39;The relationship was generally warm under the Russian President Boris Yeltsin (1991–99) until the NATO bombing of the Federal Republic of Yugoslavia in the spring of 1999, and has since deteriorated significantly. &#39;, &#34;In 2014, relations greatly deteriorated further due to the crisis in Ukraine, Russia&#39;s annexation of Crimea in 2014, differences regarding Russian military intervention in the Syrian Civil War, and from the end of 2016 over Russia&#39;s alleged interference in the 2016 U. S. elections. &#34;, &#39;Mutual sanctions imposed in 2014 remain in place. &#39;][0. 9242598414421082, 0. 9835892915725708, 0. 01635131798684597, 0. 013922506012022495]  Incorrectly classified relationships :       # Sentencesprint(sent_all_df[sent_all_df[&#39;Country_Pair&#39;] == &#39;China_United States&#39;][&#39;Summary_Sentence&#39;]. tail(5). tolist())# Sentiment probabilitiessent_all_df[sent_all_df[&#39;Country_Pair&#39;] == &#39;China_United States&#39;][&#39;Predictions_Probas&#39;]. tail(5). tolist()  [&#34;Despite tensions during his term, the Chinese population&#39;s favorability of the U. S. stood at 51% in Obama&#39;s last year of 2016, only to dip during the Trump Administration. &#34;, &#39;The relations deteriorated sharply under President Donald Trump, whose administration launched a trade war against China, banned US companies from selling equipment to Huawei, increased visa restrictions on Chinese nationality students and scholars and designated China as a &#34;currency manipulator&#34;. &#39;, &#39;During the Trump administration, and especially since the US-China trade war began, political observers have started to warn that a new cold war is emerging. &#39;, &#39;Michael D. Swaine warned in 2019, &#34;The often positive and optimistic forces, interests, and beliefs that sustained bilateral ties for decades are giving way to undue pessimism, hostility, and a zero-sum mindset in almost every area of engagement. &#34;&#39;, &#39;However by 2020, the two countries have started to take steps in order to repair the relations; U. S. lifted its currency manipulator designation on China in 13 January 2020, and both sides signed the US–China Phase One trade deal in 15 January. &#39;][0. 17708182334899902, 0. 7658159732818604, 0. 10857371240854263, 0. 7851555347442627, 0. 1416555643081665]  Our model can definitely do better! It will be great if we could have more training data if time permits! Visualizing our predictions! Our aim in this section is for us to visualize the sentiments on a world map plot. We will allow a user to select a country to focus on. And the sentiments of its relationships with other countries will be displayed on the plot. By default we will allow the user to choose a country from column Country_A. Currently our dataframe of predictions comprises of one-directional relationships. For example, if United States - China was represented in the dataframe, the China - United States relationship would be missing. Let's stack the dataframe we have currently, with another dataframe where the names in columns Country_A and Country_B are swapped. Creating a stacked dataframe : Great! Now all of our relationships are represented!       sent_all_swap_df = sent_all_df. copy()sent_all_swap_df[&#39;Country_A&#39;] = sent_all_df[&#39;Country_B&#39;]sent_all_swap_df[&#39;Country_B&#39;] = sent_all_df[&#39;Country_A&#39;]sent_all_stack_df = pd. concat([sent_all_df, sent_all_swap_df], axis=0)sent_all_stack_df. sort_values(by=&#39;Country_A&#39;, ascending=False)sent_all_stack_df[sent_all_stack_df[&#39;Country_Pair&#39;]. str. contains(&#39;China_United States&#39;)]           Country_A   Country_B   Country_Pair   Label   Summary_Sentence   Predictions_Probas         2436   China   United States   China_United States   NaN   China–United States relations (simplified Chin. . .    0. 651927       2437   China   United States   China_United States   NaN   The history of the relationship can be traced . . .    0. 987907       2438   China   United States   China_United States   NaN   The relationship between the two countries hav. . .    0. 985974       2439   China   United States   China_United States   NaN   Both countries used to have an extremely exten. . .    0. 517934       2440   China   United States   China_United States   NaN   It is a relationship of economic cooperation, . . .    0. 399313       . . .    . . .    . . .    . . .    . . .    . . .    . . .        6048   United States   China   China_United States   NaN   Despite tensions during his term, the Chinese . . .    0. 177082       6049   United States   China   China_United States   NaN   The relations deteriorated sharply under Presi. . .    0. 765816       6050   United States   China   China_United States   NaN   During the Trump administration, and especiall. . .    0. 108574       6051   United States   China   China_United States   NaN   Michael D. Swaine warned in 2019,  The often p. . .    0. 785156       6052   United States   China   China_United States   NaN   However by 2020, the two countries have starte. . .    0. 141656   100 rows × 6 columns   Calculating the average sentiment score : As we have calculated the sentiment score at a sentence level,we will use pandas' groupby to average the sentiment score for each country pair.       sent_all_stack_df. groupby(&#39;Country_Pair&#39;). mean()[&#39;Predictions_Probas&#39;]  Country_PairAbkhazia_Nauru      0. 938478Abkhazia_Nicaragua    0. 757214Abkhazia_Tuvalu     0. 961020Abkhazia_Vanuatu     0. 523380Afghanistan_China    0. 813562              . . .  United States_Vietnam  0. 910205United States_Yemen   0. 086554United States_Zambia   0. 740769Yemen_European Union   0. 984979Zambia_Zimbabwe     0. 635722Name: Predictions_Probas, Length: 1171, dtype: float64  Retrieving the ISO code for each country : Plotly, the visualization package that we will be using, identifies each country's location on the world map by using the ISO code. We can easily retrieve the ISO codes in the form of a txt file from the web. However, once again, the names of the countries used in the ISO codes txt file might be different from what we have in the dataframe. Fret not! get_std_names is here to save the day. We will map the names of the countries in the txt file to obtain its standardized std names.       country_ISO_raw_df = pd. read_csv(&#39;. /data/Wiki/country_ISO. txt&#39;, sep=&quot; &quot;,                 header=None, names=[&#39;ISO_Code&#39;, &#39;Country&#39;])country_ISO_df = get_std_names(country_ISO_raw_df,                &#39;Country&#39;, alt_names_to_countries)country_ISO_df. sort_values(by=[&#39;Country&#39;]). head(10)  /usr/local/lib/python3. 6/dist-packages/ipykernel_launcher. py:2: ParserWarning: Falling back to the &#39;python&#39; engine because the &#39;c&#39; engine does not support regex separators (separators &gt; 1 char and different from &#39;\s+&#39; are interpreted as regex); you can avoid this warning by specifying engine=&#39;python&#39;.           ISO_Code   Country   Country_Std         1   AFG   Afghanistan   afghanistan       5   ALB   Albania   albania       64   DZA   Algeria   algeria       10   ASM   American Samoa   american_samoa       6   AND   Andorra   andorra       2   AGO   Angola   angola       3   AIA   Anguilla   anguilla       11   ATA   Antarctica   antarctica       13   ATG   Antigua and Barbuda   antigua_and_barbuda       8   ARG   Argentina   argentina     We will convert the dataframe into a dictionary for faster mappings.       country_to_ISO = {k:v for k,v in zip(country_ISO_df[&#39;Country_Std&#39;], \                   country_ISO_df[&#39;ISO_Code&#39;])}    Let's create an additional column in our sent_all_stack_df to populate the ISO codes of the countries.       def get_ISO(country): try:  ISO = country_to_ISO[country] except:  ISO = &#39;Not Available&#39; return ISO          sent_all_stack_df = get_std_names(sent_all_stack_df, &#39;Country_A&#39;, \                 alt_names_to_countries)sent_all_stack_df = get_std_names(sent_all_stack_df, &#39;Country_B&#39;, \                 alt_names_to_countries)sent_all_stack_df[&#39;country_A_ISO&#39;] = sent_all_stack_df[&#39;Country_A_Std&#39;]. apply(get_ISO)sent_all_stack_df[&#39;country_B_ISO&#39;] = sent_all_stack_df[&#39;Country_B_Std&#39;]. apply(get_ISO)    We will exclude countries with no ISO codes for now.       sent_all_stack_avail_df = sent_all_stack_df[sent_all_stack_df[&#39;country_A_ISO&#39;] != &#39;Not Available&#39;]    Time to visualize the sentiments! Here are countries that we can choose from.       np. sort(sent_all_stack_avail_df[&#39;Country_A_Std&#39;]. unique())  array([&#39;afghanistan&#39;, &#39;albania&#39;, &#39;algeria&#39;, &#39;andorra&#39;, &#39;angola&#39;,    &#39;antigua_and_barbuda&#39;, &#39;argentina&#39;, &#39;armenia&#39;, &#39;australia&#39;,    &#39;austria&#39;, &#39;azerbaijan&#39;, &#39;bahamas&#39;, &#39;bahrain&#39;, &#39;bangladesh&#39;,    &#39;barbados&#39;, &#39;belarus&#39;, &#39;belgium&#39;, &#39;belize&#39;, &#39;benin&#39;, &#39;bhutan&#39;,    &#39;bolivia&#39;, &#39;bosnia_and_herzegovina&#39;, &#39;botswana&#39;, &#39;brazil&#39;,    &#39;bulgaria&#39;, &#39;burkina_faso&#39;, &#39;burundi&#39;, &#39;cambodia&#39;, &#39;cameroon&#39;,    &#39;canada&#39;, &#39;cape_verde&#39;, &#39;central_african_republic&#39;, &#39;chad&#39;,    &#39;chile&#39;, &#39;china&#39;, &#39;colombia&#39;, &#39;comoros&#39;, &#39;costa_rica&#39;, &#39;croatia&#39;,    &#39;cuba&#39;, &#39;cyprus&#39;, &#39;czech_republic&#39;, &#39;denmark&#39;, &#39;djibouti&#39;,    &#39;dominica&#39;, &#39;dominican_republic&#39;, &#39;east_timor&#39;, &#39;ecuador&#39;, &#39;egypt&#39;,    &#39;el_salvador&#39;, &#39;equatorial_guinea&#39;, &#39;eritrea&#39;, &#39;estonia&#39;,    &#39;eswatini&#39;, &#39;ethiopia&#39;, &#39;fiji&#39;, &#39;finland&#39;, &#39;france&#39;, &#39;gabon&#39;,    &#39;gambia&#39;, &#39;georgia&#39;, &#39;germany&#39;, &#39;ghana&#39;, &#39;greece&#39;, &#39;grenada&#39;,    &#39;guatemala&#39;, &#39;guinea-bissau&#39;, &#39;guyana&#39;, &#39;haiti&#39;, &#39;holy_see&#39;,    &#39;honduras&#39;, &#39;hungary&#39;, &#39;iceland&#39;, &#39;india&#39;, &#39;indonesia&#39;, &#39;iran&#39;,    &#39;iraq&#39;, &#39;ireland&#39;, &#39;israel&#39;, &#39;italy&#39;, &#39;ivory_coast&#39;, &#39;jamaica&#39;,    &#39;japan&#39;, &#39;jordan&#39;, &#39;kazakhstan&#39;, &#39;kenya&#39;, &#39;kiribati&#39;, &#39;kuwait&#39;,    &#39;kyrgyzstan&#39;, &#39;latvia&#39;, &#39;lebanon&#39;, &#39;lesotho&#39;, &#39;liberia&#39;, &#39;libya&#39;,    &#39;liechtenstein&#39;, &#39;lithuania&#39;, &#39;luxembourg&#39;, &#39;madagascar&#39;, &#39;malawi&#39;,    &#39;malaysia&#39;, &#39;maldives&#39;, &#39;mali&#39;, &#39;malta&#39;, &#39;marshall_islands&#39;,    &#39;mauritania&#39;, &#39;mauritius&#39;, &#39;mexico&#39;, &#39;moldova&#39;, &#39;monaco&#39;,    &#39;mongolia&#39;, &#39;montenegro&#39;, &#39;morocco&#39;, &#39;mozambique&#39;, &#39;myanmar&#39;,    &#39;namibia&#39;, &#39;nauru&#39;, &#39;nepal&#39;, &#39;netherlands&#39;, &#39;new_zealand&#39;,    &#39;nicaragua&#39;, &#39;niger&#39;, &#39;nigeria&#39;, &#39;north_macedonia&#39;, &#39;norway&#39;,    &#39;oman&#39;, &#39;pakistan&#39;, &#39;palau&#39;, &#39;panama&#39;, &#39;papua_new_guinea&#39;,    &#39;paraguay&#39;, &#39;peru&#39;, &#39;philippines&#39;, &#39;poland&#39;, &#39;portugal&#39;, &#39;qatar&#39;,    &#39;republic_of_congo&#39;, &#39;romania&#39;, &#39;russia&#39;, &#39;rwanda&#39;, &#39;saint_lucia&#39;,    &#39;saint_vincent_and_grenadines&#39;, &#39;samoa&#39;, &#39;san_marino&#39;,    &#39;saudi_arabia&#39;, &#39;senegal&#39;, &#39;serbia&#39;, &#39;seychelles&#39;, &#39;sierra_leone&#39;,    &#39;singapore&#39;, &#39;slovakia&#39;, &#39;slovenia&#39;, &#39;solomon_islands&#39;, &#39;somalia&#39;,    &#39;south_africa&#39;, &#39;south_korea&#39;, &#39;south_sudan&#39;, &#39;spain&#39;, &#39;sri_lanka&#39;,    &#39;sudan&#39;, &#39;suriname&#39;, &#39;sweden&#39;, &#39;switzerland&#39;, &#39;taiwan&#39;,    &#39;tajikistan&#39;, &#39;tanzania&#39;, &#39;thailand&#39;, &#39;togo&#39;, &#39;tonga&#39;,    &#39;trinidad_and_tobago&#39;, &#39;tunisia&#39;, &#39;turkey&#39;, &#39;turkmenistan&#39;,    &#39;tuvalu&#39;, &#39;uganda&#39;, &#39;ukraine&#39;, &#39;united_arab_emirates&#39;,    &#39;united_kingdom&#39;, &#39;united_states&#39;, &#39;uruguay&#39;, &#39;uzbekistan&#39;,    &#39;vanuatu&#39;, &#39;venezuela&#39;, &#39;vietnam&#39;, &#39;yemen&#39;, &#39;zambia&#39;, &#39;zimbabwe&#39;],   dtype=object)  For illustration purposes, let's select  united_states        country = &quot;united_states&quot;sent_country_df = sent_all_stack_avail_df[sent_all_stack_avail_df[&#39;Country_A_Std&#39;] == country]    With the help of Plotly, we are able to illustrate the sentiments on a world map.       # https://plot. ly/python/bubble-maps/import plotly. express as pxfig = px. scatter_geo(sent_country_df,           locations=&quot;country_B_ISO&quot;,           color=&quot;Predictions_Probas&quot;,           hover_name=&quot;Country_B_Std&quot;,           projection=&quot;natural earth&quot;)# To show smaller countriesfig. update_layout({  &#39;geo&#39;: {    &#39;resolution&#39;: 50  }})fig. show()  WARNING:tensorflow:Unresolved object in checkpoint: (root). optimizer. iterWARNING:tensorflow:A checkpoint was restored (e. g. tf. train. Checkpoint. restore or tf. keras. Model. load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e. g. tf. train. Checkpoint. restore(. . . ). expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www. tensorflow. org/alpha/guide/checkpoints#loading_mechanics for details. WARNING:tensorflow:Unresolved object in checkpoint: (root). optimizer. iterWARNING:tensorflow:A checkpoint was restored (e. g. tf. train. Checkpoint. restore or tf. keras. Model. load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e. g. tf. train. Checkpoint. restore(. . . ). expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www. tensorflow. org/alpha/guide/checkpoints#loading_mechanics for details.                                         Future improvements ALBERT is indeed a very impressive language model. I'm looking forward to replicate the success Albert has on the Wikipedia dataset to news headlines scrapped from the web. However, there are added layers of complexity that needs to be solved: As explained earlier, we need to be able to differentiate facts from commentaries. A simple Googling of  United States - China  relations might not return us results that are all relevant. An added element of effort needs to be put in to clean the dataset. Like any data scientist would do, a deeper analysis of the results can be conducted by analysing the precision/recall on the test dataset. That's it for now folks! If you have any questions/comments please feel free to reach out to me through Linkedin or email at ken. wangtm@gmail. com "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')
    this.metadataWhitelist = ['position']

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}